{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bpratham2001/ResNet/blob/main/Mass_Regressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mass Regression\n",
        "\n",
        "Please train a model to estimate (regress) the mass of the particle based on particle images using the provided dataset.\n",
        "\n",
        "DataSet Description: 125x125 image matrices with name of variables: ieta and iphi, with 4 channels called X_jet (Track pT, DZ and D0, ECAL). Please use at least ECAL and Track pT channels and ‘am‘ as the target feature. If there are more than 4 channels in the dataset then you should use X_jet (Track pT, DZ and D0, ECAL) only. Please train your model on 80% of the data and evaluate on the remaining 20%. Please make sure not to overfit on the test dataset - it will be checked with an independent sample.\n",
        "\n",
        "Datasets: https://cernbox.cern.ch/s/zUvpkKhXIp0MJ0g\n",
        "\n",
        "**1,051,917 rows in total**"
      ],
      "metadata": {
        "id": "3HkhA62p43xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title !wget .parquet files\n",
        "\n",
        "#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_0.parquet\n",
        "#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_1.parquet\n",
        "#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_2.parquet\n",
        "#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_3.parquet\n",
        "#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_4.parquet\n",
        "#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_5.parquet\n",
        "#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_6.parquet\n",
        "\n",
        "parqs = ['top_gun_opendata_0.parquet', 'top_gun_opendata_1.parquet', 'top_gun_opendata_2.parquet',\n",
        "         'top_gun_opendata_3.parquet', 'top_gun_opendata_4.parquet', 'top_gun_opendata_5.parquet',\n",
        "         'top_gun_opendata_6.parquet']\n",
        "#folder = '/content/drive/My Drive/top_gun/'\n",
        "folder = '/kaggle/working/'"
      ],
      "metadata": {
        "id": "-FPBtEKcRHhP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:26.958472Z",
          "iopub.execute_input": "2025-03-25T12:06:26.959092Z",
          "iopub.status.idle": "2025-03-25T12:06:26.966724Z",
          "shell.execute_reply.started": "2025-03-25T12:06:26.959060Z",
          "shell.execute_reply": "2025-03-25T12:06:26.965946Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title imports and installs\n",
        "\n",
        "#import pyspark\n",
        "#from pyspark.sql import SparkSession\n",
        "#import pandas as pd\n",
        "#from IPython.display import clear_output\n",
        "import random\n",
        "import numpy as np\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import datetime\n",
        "from math import sqrt\n",
        "import gc\n",
        "#from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fCyVKUnYhvTD",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:26.967383Z",
          "iopub.execute_input": "2025-03-25T12:06:26.967566Z",
          "iopub.status.idle": "2025-03-25T12:06:28.798999Z",
          "shell.execute_reply.started": "2025-03-25T12:06:26.967547Z",
          "shell.execute_reply": "2025-03-25T12:06:28.797270Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Model Architecture (ResNet)\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "  # This is the residual block, there are 4 residual blocks in ResNet15\n",
        "  def __init__(self, in_channels, out_channels, stride=1):\n",
        "    super(ResBlock, self).__init__()\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=stride, padding=0, bias=False)\n",
        "    nn.init.kaiming_normal_(self.conv1.weight)\n",
        "    self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    nn.init.kaiming_normal_(self.conv2.weight)\n",
        "    self.bn2 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "    self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "    nn.init.kaiming_normal_(self.conv3.weight)\n",
        "    self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      shortcut_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, bias=False)\n",
        "      nn.init.kaiming_normal_(shortcut_conv.weight)\n",
        "      self.shortcut.append(shortcut_conv)\n",
        "      self.shortcut.append(nn.BatchNorm2d(out_channels))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.conv3(out)\n",
        "    out = self.bn3(out)\n",
        "    out += self.shortcut(x)\n",
        "    out = self.relu(out)\n",
        "    return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, num_classes=2):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(4, 16, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "    nn.init.kaiming_normal_(self.conv1.weight)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    self.res1 = ResBlock(16, 32)\n",
        "    self.res2 = ResBlock(32, 64)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "    nn.init.kaiming_normal_(self.conv2.weight)\n",
        "    self.bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "    self.res3 = ResBlock(128, 256)\n",
        "    self.res4 = ResBlock(256, 512)\n",
        "\n",
        "    self.fc1 = nn.Linear(512 * 125 * 125, 1)\n",
        "    nn.init.kaiming_normal_(self.fc1.weight)\n",
        "    #self.fc2 = nn.Linear(16 * 125 * 125, 1)\n",
        "    #nn.init.kaiming_normal_(self.fc2.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.maxpool(out)\n",
        "    out = self.res1(out)\n",
        "    out = self.res2(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out = self.res3(out)\n",
        "    out = self.res4(out)\n",
        "    out = torch.flatten(out, 1)\n",
        "    out = self.fc1(out)\n",
        "    #out = self.fc2(out)\n",
        "    out = self.relu(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "K6JsswyMUuzm",
        "cellView": "form",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:28.799997Z",
          "iopub.execute_input": "2025-03-25T12:06:28.800322Z",
          "iopub.status.idle": "2025-03-25T12:06:28.814170Z",
          "shell.execute_reply.started": "2025-03-25T12:06:28.800297Z",
          "shell.execute_reply": "2025-03-25T12:06:28.812883Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title dataset metadata and helper functions\n",
        "\n",
        "def get_parquet_rows(parq, start=0, end=-1, drive=False):\n",
        "  # since rows == row_groups in the dataset, we can use them interchangeably\n",
        "  lst = []\n",
        "  if drive:\n",
        "    parq_file = pq.ParquetFile(folder+parq)\n",
        "  else:\n",
        "    parq_file = pq.ParquetFile(parq)\n",
        "  if end < 0:\n",
        "    end = parq_file.scan_contents()\n",
        "  while start < end:\n",
        "    lst.append(parq_file.read_row_group(start))\n",
        "    start += 1\n",
        "  return lst\n",
        "\n",
        "def make_X(X_jet):\n",
        "  X_set = np.zeros(shape=(4, 125, 125))\n",
        "  for i in range(4): # only selecting the first 4 channels (Track pT, DZ, D0, ECAL)\n",
        "    for j in range(125):\n",
        "      for k in range(125):\n",
        "        X_set[i][j][k] = X_jet[i][j][k].astype(float)\n",
        "  return X_set\n",
        "  #return torch.tensor(X_set, dtype=torch.float32)\n",
        "\n",
        "def make_Xs(X_jets, chunk):\n",
        "  X_sets = np.zeros(shape=(chunk, 4, 125, 125))\n",
        "  for i in range(chunk):\n",
        "    for j in range(4):\n",
        "      for k in range(125):\n",
        "        for l in range(125):\n",
        "          X_sets[i][j][k][l] = X_jets[i][j][k][l].astype(float)\n",
        "  return X_sets\n",
        "\n",
        "\n",
        "def get_Xy(parq, start=0, end=-1, drive=False):\n",
        "  arr = np.array(get_parquet_rows(parq, start, end, drive))\n",
        "  X = []\n",
        "  y = []\n",
        "  for i in range(len(arr)):\n",
        "    X.append(make_X(np.array(arr[i][0][0])))\n",
        "    y.append(arr[i][0][1])\n",
        "  return np.array(X), np.array(y)\n",
        "\n",
        "\"\"\" # row counter\n",
        "tot = 0\n",
        "for parq in parqs:\n",
        "  num = pq.ParquetFile(folder+parq).scan_contents()\n",
        "  tot += num\n",
        "  print(parq+\" contains \"+str(num)+\" rows\")\n",
        "print(\"total \" + str(tot))\n",
        "del tot, num\n",
        "\n",
        "top_gun_opendata_0.parquet contains 150327 rows\n",
        "top_gun_opendata_1.parquet contains 150165 rows\n",
        "top_gun_opendata_2.parquet contains 150451 rows\n",
        "top_gun_opendata_3.parquet contains 150448 rows\n",
        "top_gun_opendata_4.parquet contains 150557 rows\n",
        "top_gun_opendata_5.parquet contains 150056 rows\n",
        "top_gun_opendata_6.parquet contains 149913 rows\n",
        "total 1051917\n",
        "\"\"\"\n",
        "\n",
        "for parq in parqs:\n",
        "  break\n",
        "  print(pq.read_metadata(folder+parq))\n",
        "  #print(pq.ParquetFile(folder+parq).read_row_group(0))\n",
        "\n",
        "class Deck():\n",
        "  def __init__(self, filenames, filesizes):\n",
        "    self.filenames = filenames\n",
        "    self.filesizes = filesizes\n",
        "    self.deck = []\n",
        "\n",
        "  def create_deck(self, max_chunk):\n",
        "    self.deck = []\n",
        "    for i in range(len(self.filenames)):\n",
        "      start = 0\n",
        "      while self.filesizes[i] > 0:\n",
        "        if max_chunk >= self.filesizes[i]:\n",
        "          self.deck.append([self.filenames[i], start, (start+self.filesizes[i])])\n",
        "          self.filesizes[i] = 0\n",
        "        else:\n",
        "          self.deck.append([self.filenames[i], start, (start+max_chunk)])\n",
        "          self.filesizes[i] -= max_chunk\n",
        "          start += max_chunk\n",
        "\n",
        "  def shuffle_deck(self):\n",
        "    random.shuffle(self.deck)\n",
        "\n",
        "  def deal_cards(self, split):\n",
        "    return self.deck[:int(len(self.deck)*split)], self.deck[int(len(self.deck)*split):]\n",
        "\n",
        "  def play_card(self, card, batch_size, drive=False, reshuffle=True):\n",
        "    if drive:\n",
        "      parq_file = pq.ParquetFile(folder+card[0])\n",
        "    else:\n",
        "      parq_file = pq.ParquetFile(card[0])\n",
        "\n",
        "    X = parq_file.read_row_groups(np.arange(card[1], card[2]), columns=['X_jet', 'm']).to_pandas()\n",
        "    torch.cuda.empty_cache()\n",
        "    y = torch.tensor(X['m'].to_numpy(), dtype=torch.float32)\n",
        "    X = make_Xs(X['X_jet'].to_numpy(), card[2]-card[1]) # Works, but slow\n",
        "    #X_tensor = torch.from_numpy(df['X_jet'].to_numpy(dtype=float))\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    #print(\"X_tensor\", str(X.shape))\n",
        "    #print(\"y_tensor\", str(y.shape))\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    return DataLoader(TensorDataset(X, y), batch_size=batch_size, shuffle=reshuffle)"
      ],
      "metadata": {
        "id": "hVZysWXXTHAb",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:28.815344Z",
          "iopub.execute_input": "2025-03-25T12:06:28.815567Z",
          "iopub.status.idle": "2025-03-25T12:06:34.778634Z",
          "shell.execute_reply.started": "2025-03-25T12:06:28.815545Z",
          "shell.execute_reply": "2025-03-25T12:06:34.777401Z"
        },
        "outputId": "bc27bc38-0d1b-4ab9-c6f1-cabbd47e3159"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<pyarrow._parquet.FileMetaData object at 0x7de0b29106d0>\n  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n  num_columns: 5\n  num_rows: 150327\n  num_row_groups: 150327\n  format_version: 1.0\n  serialized_size: 72239212\n<pyarrow._parquet.FileMetaData object at 0x7de0b2ac0ae0>\n  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n  num_columns: 5\n  num_rows: 150165\n  num_row_groups: 150165\n  format_version: 1.0\n  serialized_size: 72161492\n<pyarrow._parquet.FileMetaData object at 0x7de0b2aaa930>\n  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n  num_columns: 5\n  num_rows: 150451\n  num_row_groups: 150451\n  format_version: 1.0\n  serialized_size: 72300895\n<pyarrow._parquet.FileMetaData object at 0x7de0b29106d0>\n  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n  num_columns: 5\n  num_rows: 150448\n  num_row_groups: 150448\n  format_version: 1.0\n  serialized_size: 72299402\n<pyarrow._parquet.FileMetaData object at 0x7de0b292a750>\n  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n  num_columns: 5\n  num_rows: 150557\n  num_row_groups: 150557\n  format_version: 1.0\n  serialized_size: 72352622\n<pyarrow._parquet.FileMetaData object at 0x7de0b2aaa930>\n  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n  num_columns: 5\n  num_rows: 150056\n  num_row_groups: 150056\n  format_version: 1.0\n  serialized_size: 72110141\n<pyarrow._parquet.FileMetaData object at 0x7de0b2ac0ae0>\n  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n  num_columns: 5\n  num_rows: 149913\n  num_row_groups: 149913\n  format_version: 1.0\n  serialized_size: 72040028\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from multiprocessing import Pool\n",
        "print(datetime.datetime.now())\n",
        "parq_file = pq.ParquetFile(folder+parqs[0])\n",
        "df = parq_file.read_row_groups(np.arange(0, 4000), columns=['X_jet', 'm']).to_pandas()\n",
        "X = []\n",
        "with Pool(4) as p:\n",
        "  X.append(p.map(make_X, df['X_jet'].values))\n",
        "  p.close()\n",
        "  p.join()\n",
        "len(X)\n",
        "#X = torch.tensor(make_Xs(df['X_jet'].values, 4000), dtype=torch.float32) #10min\n",
        "print(datetime.datetime.now())\n",
        "\"\"\"\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fl2gflwzXLN",
        "outputId": "aad46003-d2b3-4fd3-96fa-cf5fc06d2435",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:34.779738Z",
          "iopub.execute_input": "2025-03-25T12:06:34.779985Z",
          "iopub.status.idle": "2025-03-25T12:06:39.053870Z",
          "shell.execute_reply.started": "2025-03-25T12:06:34.779961Z",
          "shell.execute_reply": "2025-03-25T12:06:39.052661Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: tensorflow-cpu in /usr/local/lib/python3.10/site-packages (2.19.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (3.8.0)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (0.6.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (3.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (0.2.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.5.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.70.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (18.1.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (24.2)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (25.2.10)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (3.13.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.17.2)\nRequirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.0.2)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.6.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.32.3)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.17.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (4.12.2)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.1.0)\nRequirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.19.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (75.8.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (5.29.3)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (0.37.1)\nRequirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (0.5.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-cpu) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow-cpu) (13.9.4)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow-cpu) (0.14.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow-cpu) (0.0.8)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2025.1.31)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow-cpu) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow-cpu) (3.1.3)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow-cpu) (3.7)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow-cpu) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow-cpu) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow-cpu) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-cpu) (0.1.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Tuning and Loading the Model\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "#import torch_xla.distributed.parallel_loader as pl\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = xm.xla_device()\n",
        "###############################################################################\n",
        "epochs = 2\n",
        "split = 0.8\n",
        "max_chunk = 10000\n",
        "batch_size = 250\n",
        "optimiseur = ['Adam', 0.001 * xm.xrt_world_size()] # ['Adam, learning rate'] / ['SGD', learning rate, momentum] // scaled_eta = eta * xm.xrt_world_size()\n",
        "criterion = nn.MSELoss()#.to(device)\n",
        "path = '' # 'mass_regressor.pth'\n",
        "###############################################################################\n",
        "\n",
        "model = ResNet()\n",
        "mx = xmp.MpModelWrapper(model)\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "  print(\"Using \", torch.cuda.device_count(), \"GPUs\")\n",
        "  model = nn.DataParallel(model)\n",
        "\n",
        "if path != '':\n",
        "  model.load_state_dict(torch.load(f=path))"
      ],
      "metadata": {
        "id": "6Ei1S3sJT3NG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:39.055255Z",
          "iopub.execute_input": "2025-03-25T12:06:39.055562Z",
          "iopub.status.idle": "2025-03-25T12:06:44.766945Z",
          "shell.execute_reply.started": "2025-03-25T12:06:39.055532Z",
          "shell.execute_reply": "2025-03-25T12:06:44.765877Z"
        },
        "outputId": "77304aad-ece3-45f0-b485-5112dbb0b689"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1742904401.773716   10989 common_lib.cc:621] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:239\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title averageMeter\n",
        "class AverageMeter(object):\n",
        "\n",
        "    '''Computes and stores the average and current value'''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val   = 0\n",
        "        self.avg   = 0\n",
        "        self.sum   = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n = 1):\n",
        "        self.val    = val\n",
        "        self.sum   += val * n\n",
        "        self.count += n\n",
        "        self.avg    = self.sum / self.count"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rddg_wYUElIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = model.to(device)\n",
        "model  = mx.to(device)\n",
        "\n",
        "if optimiseur[0] == 'Adam':\n",
        "  optimiseur = torch.optim.Adam(model.parameters(),lr=optimiseur[1])\n",
        "elif optimiseur[0] == 'SGD':\n",
        "  optimiseur = torch.optim.SGD(model.parameters(), lr=optimiseur[1], momentum=optimiseur[2])\n",
        "\n",
        "param_size = 0\n",
        "for param in model.parameters():\n",
        "  param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in model.buffers():\n",
        "  buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "print('model size: {:.3f}MB'.format((param_size + buffer_size) / 1024**2))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:44.768314Z",
          "iopub.execute_input": "2025-03-25T12:06:44.768656Z",
          "iopub.status.idle": "2025-03-25T12:06:44.798189Z",
          "shell.execute_reply.started": "2025-03-25T12:06:44.768630Z",
          "shell.execute_reply": "2025-03-25T12:06:44.796610Z"
        },
        "id": "noMO4GP8C2q4",
        "outputId": "0b6e27a4-c3bf-44e9-82c3-7fa899a53703"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "model size: 35.034MB\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training\n",
        "\n",
        "# para_loader = pl.ParallelLoader(valid_loader, [device])\n",
        "# para_loader = para_loader.per_device_loader(device)\n",
        "\n",
        "def train(model):\n",
        "  print(\"Training started at\", datetime.datetime.now())\n",
        "  parq_len = [150327, 150165, 150451, 150448, 150557, 150056, 149913]\n",
        "  deck = Deck(parqs, parq_len)\n",
        "  deck.create_deck(max_chunk)\n",
        "  deck.shuffle_deck()\n",
        "  train_cards, test_cards = deck.deal_cards(split)\n",
        "  plotloss = []\n",
        "  flag = False\n",
        "  model.train()\n",
        "  trn_loss_meter = AverageMeter()\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimiseur, step_size = 1, gamma = 0.5)\n",
        "\n",
        "  gc.collect()\n",
        "  for epoch in range(epochs):\n",
        "    \"\"\"\n",
        "    if (epochs > 5) and (epoch > (epochs*0.5)):\n",
        "      optimiseur = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "    elif (epochs > 5) and (epoch > (epochs*0.75)):\n",
        "      optimiseur = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
        "    \"\"\"\n",
        "    #running_loss = 0.0 # MSE, per epoch\n",
        "\n",
        "    for card in train_cards:\n",
        "      #acc = [] # RMSE, per card per epoch\n",
        "      #print(card)\n",
        "      for i, (inputs, labels) in enumerate(deck.play_card(card, batch_size, drive=True, reshuffle=False)):\n",
        "        #print('{}MB'.format(inputs.nelement() * inputs.element_size() / 1024**2))\n",
        "        #print('{}MB'.format(labels.nelement() * labels.element_size() / 1024**2))\n",
        "        outputs = model(inputs)\n",
        "        if flag:\n",
        "          print(outputs) #torch.flatten(torch.rot90(outputs)))\n",
        "          print(labels) #torch.flatten(labels))\n",
        "          flag = False\n",
        "        loss = criterion(outputs, labels.unsqueeze(1))\n",
        "        optimiseur.zero_grad()\n",
        "        loss.backward()\n",
        "        #optimiseur.step()\n",
        "        xm.optimizer_step(optimiseur, barrier=True) # barrier is required on single-core training but can be dropped with multiple cores\n",
        "        #running_loss += loss.item()\n",
        "        trn_loss_meter.update(loss.detach().item(), inputs.size(0))\n",
        "        #acc.append(sqrt(loss))\n",
        "      #print(f\"RMSE: {sum(acc)/len(acc)}\")\n",
        "      xm.master_print('E{}, {} | loss = {:.6f}, avg_loss = {:.6f}'.format(epoch, card, loss.item(), trn_loss_meter.avg))\n",
        "\n",
        "      del inputs, labels, outputs, loss\n",
        "      gc.collect()\n",
        "      scheduler.step()\n",
        "    plotloss.append(trn_loss_meter.avg)\n",
        "    #print(f\"E [{epoch + 1}/{epochs}], L: {running_loss:.4f}, T: [{datetime.datetime.now()}]\")\n",
        "\n",
        "  print(\"Training completed at\", datetime.datetime.now())\n",
        "  return plotloss\n",
        "\n",
        "# wrapper function\n",
        "def _mp_fn(rank, flags):\n",
        "  torch.set_default_tensor_type('torch.FloatTensor')\n",
        "  trn_losses = train(model)\n",
        "  np.save('trn_losses.npy', np.array(trn_losses))\n",
        "  #np.save('val_losses.npy', np.array(val_losses))\n",
        "\n",
        "# modeling\n",
        "gc.collect()\n",
        "FLAGS = {}\n",
        "xmp.spawn(_mp_fn, args = (FLAGS,), nprocs = 8, start_method = 'fork')"
      ],
      "metadata": {
        "id": "e-3iwoTfj_b-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:44.799452Z",
          "iopub.execute_input": "2025-03-25T12:06:44.799787Z",
          "iopub.status.idle": "2025-03-25T12:24:26.833909Z",
          "shell.execute_reply.started": "2025-03-25T12:06:44.799747Z",
          "shell.execute_reply": "2025-03-25T12:24:26.832156Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(x=np.array(list(range(epochs))), y=np.array(plotloss))"
      ],
      "metadata": {
        "id": "kzbPFbadz4rj",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Testing\n",
        "\n",
        "print(\"Testing started at\", datetime.datetime.now())\n",
        "\n",
        "val_loss = 0.0\n",
        "acc = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for card in test_cards:\n",
        "    for i, (inputs, labels) in enumerate(deck.play_card(card, batch_size, drive=True, reshuffle=False)):\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels.unsqueeze(1))\n",
        "      val_loss += loss.item()\n",
        "      acc.append(sqrt(loss))\n",
        "    print(sqrt(loss))\n",
        "\n",
        "print(f\"avg RMSE: {sum(acc)/len(acc)}\")\n",
        "print(\"Testing completed at\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "vkxrVrm7sqU7",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Saving\n",
        "torch.save(obj=model.state_dict(), f=\"resnet_regressor.pth\")"
      ],
      "metadata": {
        "id": "JL2JJoDNW4m7",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorflow stuffs to ignore"
      ],
      "metadata": {
        "id": "BAD-XNWc0D0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title imports and installs\n",
        "!pip install tensorflow==2.18.0\n",
        "!pip install tensorflow-tpu==2.18.0 --find-links=https://storage.googleapis.com/libtpu-tf-releases/index.html\n",
        "#import pyspark\n",
        "#from pyspark.sql import SparkSession\n",
        "#import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "#import tensorflow_datasets as tfds\n",
        "import datetime\n",
        "from math import sqrt\n",
        "#from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "O45LVjs13VUQ",
        "cellView": "form",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Tuning\n",
        "#TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "#optimiseur = keras.optimizers.SGD(learning_rate=0.1,momentum=0.9,decay = 1e-04)\n",
        "optimiseur = keras.optimizers.Adam(learning_rate=0.01)\n",
        "criterion = keras.losses.MeanSquaredError()\n",
        "batch_size_parq = 5000\n",
        "batch_size_train = 256\n",
        "steps_per_epoch = 50000 // batch_size_train\n",
        "exec_step = 50 # between 2 and steps_per_epoch\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "yyCKXwM1K3Ak",
        "cellView": "form",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title dataset metadata and helper functions\n",
        "\n",
        "def get_parquet_rows(parq, start=0, end=-1, drive=False):\n",
        "  # since rows == row_groups in the dataset, we can use them interchangeably\n",
        "  lst = []\n",
        "  if drive:\n",
        "    parq_file = pq.ParquetFile(folder+parq)\n",
        "  else:\n",
        "    parq_file = pq.ParquetFile(parq)\n",
        "  if end < 0:\n",
        "    end = parq_file.scan_contents()\n",
        "  while start < end:\n",
        "    lst.append(parq_file.read_row_group(start))\n",
        "    start += 1\n",
        "  return lst\n",
        "\n",
        "def make_X(X_jet):\n",
        "  X_set = np.zeros(shape=(4, 125, 125))\n",
        "  for i in range(4): # only selecting the first 4 channels (Track pT, DZ, D0, ECAL)\n",
        "    for j in range(125):\n",
        "      for k in range(125):\n",
        "        X_set[i][j][k] = X_jet[i][j][k]\n",
        "  return tf.convert_to_tensor(X_set, tf.float32)\n",
        "\n",
        "def make_Xs(X_jets):\n",
        "  X_sets = np.zeros(shape=(len(X_jets), 4, 125, 125))\n",
        "  for i in range(len(X_jets)):\n",
        "    for j in range(4): # only selecting the first 4 channels (Track pT, DZ, D0, ECAL)\n",
        "      for k in range(125):\n",
        "        for l in range(125):\n",
        "          X_sets[i][j][k][l] = X_jets[i][j][k][l]\n",
        "  return tf.convert_to_tensor(X_sets, tf.float32)\n",
        "\n",
        "\n",
        "def get_Xy(parq, start=0, end=-1, drive=False):\n",
        "  arr = np.array(get_parquet_rows(parq, start, end, drive))\n",
        "  X = []\n",
        "  y = []\n",
        "  for i in range(len(arr)):\n",
        "    X.append(make_X(np.array(arr[i][0][0])))\n",
        "    y.append(arr[i][0][1])\n",
        "  return np.array(X), np.array(y)\n",
        "\n",
        "\"\"\" # row counter\n",
        "tot = 0\n",
        "for parq in parqs:\n",
        "  num = pq.ParquetFile(folder+parq).scan_contents()\n",
        "  tot += num\n",
        "  print(parq+\" contains \"+str(num)+\" rows\")\n",
        "print(\"total \" + str(tot))\n",
        "del tot, num\n",
        "\n",
        "top_gun_opendata_0.parquet contains 150327 rows\n",
        "top_gun_opendata_1.parquet contains 150165 rows\n",
        "top_gun_opendata_2.parquet contains 150451 rows\n",
        "top_gun_opendata_3.parquet contains 150448 rows\n",
        "top_gun_opendata_4.parquet contains 150557 rows\n",
        "top_gun_opendata_5.parquet contains 150056 rows\n",
        "top_gun_opendata_6.parquet contains 149913 rows\n",
        "total 1051917\n",
        "\"\"\"\n",
        "parq_len = [150327, 150165, 150451, 150448, 150557, 150056, 149913]\n",
        "\n",
        "for parq in parqs:\n",
        "  print(pq.read_metadata(folder+parq))\n",
        "  #print(pq.ParquetFile(folder+parq).read_row_group(0))\n",
        "  break\n",
        "\n",
        "\"\"\"\n",
        "print(datetime.datetime.now())\n",
        "parq_file = pq.ParquetFile(folder+parqs[0])\n",
        "for i in parq_file.iter_batches(batch_size=4000, columns=['X_jet', 'm']):\n",
        "  df = i.to_pandas()\n",
        "  df['X_jet'] = df['X_jet'].map(lambda x: make_X(x))\n",
        "  break\n",
        "print(df['X_jet'].iloc[0].shape)\n",
        "print(datetime.datetime.now())\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FrAPIhCjKdgp",
        "cellView": "form",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title tf_recordwriter\n",
        "\n",
        "def read_tfrecord(example):\n",
        "  features={\n",
        "    \"X\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"y\": tf.io.FixedLenFeature([], tf.float32),\n",
        "    }\n",
        "  example = tf.io.parse_single_example(example, features)\n",
        "  #X = make_X(example[\"X_track_pT\", \"X_DZ\", \"X_D0\", \"X_ECAL\"])\n",
        "  X = tf.io.parse_tensor(example['X'], tf.float32)\n",
        "  y = tf.cast(example['y'], tf.float32)\n",
        "  return X, y\n",
        "\n",
        "def get_datasets(tfRecords, batch_size, shuffle=5000, multiple=False):\n",
        "  #For optimal performance, read multiple TFRecord files and set option experimental_deterministic = False\n",
        "  dataset = tf.data.TFRecordDataset(tfRecords, num_parallel_reads=tf.data.experimental.AUTOTUNE)\n",
        "  option = tf.data.Options()\n",
        "  if multiple:\n",
        "    option.experimental_deterministic = False\n",
        "  dataset = dataset.with_options(option)\n",
        "  dataset = dataset.map(read_tfrecord)\n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.shuffle(shuffle)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset\n",
        "\n",
        "with tf.io.TFRecordWriter(\"top_gun.tfrecords\") as file_writer:\n",
        "  print(\"Writing started at\", datetime.datetime.now())\n",
        "  for i in range(len(parqs)):\n",
        "    for j in range(parq_len[i]):\n",
        "      X, y = get_Xy(parqs[i], start=j, end=(j+4000), drive=True)\n",
        "      X_serialized = tf.io.serialize_tensor(X)\n",
        "      j += 5000\n",
        "      features = {\n",
        "          \"X\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[X_serialized.numpy()])),\n",
        "          \"y\": tf.train.Feature(float_list=tf.train.FloatList(value=y)),\n",
        "          }\n",
        "      features = tf.train.Features(feature=features)\n",
        "      example = tf.train.Example(features=features)\n",
        "      record_bytes = example.SerializeToString()\n",
        "      file_writer.write(record_bytes)\n",
        "      print(datetime.datetime.now())\n",
        "    print(f\"parq {parq[i]} complete\")\n",
        "  print(\"Writing completed at\", datetime.datetime.now())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3DnE1SW189sR",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ResNet18 Architecture\n",
        "\"\"\"\n",
        "ResNet-18\n",
        "Reference:\n",
        "[1] K. He et al. Deep Residual Learning for Image Recognition. CVPR, 2016\n",
        "[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers:\n",
        "Surpassing human-level performance on imagenet classification. In\n",
        "ICCV, 2015.\n",
        "\"\"\"\n",
        "\n",
        "class ResBlock(Model):\n",
        "  def __init__(self, channels, down_sample=False):\n",
        "    super().__init__()\n",
        "    self.__channels = channels\n",
        "    self.__down_sample = down_sample\n",
        "    self.__strides = [2, 1] if down_sample else [1, 1]\n",
        "\n",
        "    self.conv1 = Conv2D(self.__channels, strides=self.__strides[0], kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\")\n",
        "    self.bn1 = BatchNormalization()\n",
        "    self.conv2 = Conv2D(self.__channels * 2, strides=self.__strides[1], kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\")\n",
        "    self.bn2 = BatchNormalization()\n",
        "    self.merge = Add()\n",
        "    if self.__down_sample:\n",
        "      self.res_conv = Conv2D(self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=\"he_normal\", padding=\"same\")\n",
        "      self.res_bn = BatchNormalization()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    res = inputs\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.bn1(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    if self.__down_sample:\n",
        "      res = self.res_conv(res)\n",
        "      res = self.res_bn(res)\n",
        "    x = self.merge([x, res])\n",
        "    out = tf.nn.relu(x)\n",
        "    return out\n",
        "\n",
        "\n",
        "class ResNet(Model):\n",
        "  def __init__(self, **kwargs):\n",
        "    \"\"\"num_classes: number of classes in specific classification task.\"\"\"\n",
        "    super().__init__(**kwargs)\n",
        "    self.conv1 = Conv2D(64, (7, 7), strides=2, padding=\"same\", kernel_initializer=\"he_normal\")\n",
        "    self.bn = BatchNormalization()\n",
        "\n",
        "    self.maxpool = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
        "\n",
        "    self.res1_1 = ResBlock(64)\n",
        "    self.res1_2 = ResBlock(64)\n",
        "\n",
        "    self.res2_1 = ResBlock(128, down_sample=True)\n",
        "    self.res2_2 = ResBlock(128)\n",
        "\n",
        "    self.res3_1 = ResBlock(256, down_sample=True)\n",
        "    self.res3_2 = ResBlock(256)\n",
        "\n",
        "    self.res4_1 = ResBlock(512, down_sample=True)\n",
        "    self.res4_2 = ResBlock(512)\n",
        "\n",
        "    self.avgpool = GlobalAveragePooling2D()\n",
        "\n",
        "    self.fc1 = Dense(1, activation=\"relu\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "    out = self.conv1(inputs)\n",
        "    out = self.bn(out)\n",
        "    out = tf.nn.relu(out)\n",
        "    out = self.maxpool(out)\n",
        "    for res_block in [self.res1_1, self.res1_2, self.res2_1, self.res2_2, self.res3_1, self.res3_2, self.res4_1, self.res4_2]:\n",
        "      out = res_block(out)\n",
        "    out = self.avgpool(out)\n",
        "    out = Flatten(out)\n",
        "    out = self.fc1(out)\n",
        "    return out\n",
        "\n",
        "with strategy.scope():\n",
        "  model = ResNet()\n",
        "  model.build(input_shape = (None, 125, 125, 4))\n",
        "  model.compile(optimizer=optimiseur, steps_per_execution=exec_step, loss=criterion, metrics=[\"mse\"])\n",
        "\"\"\"\n",
        "model = ResNet()\n",
        "tpu_model = tf.compat.v1.estimator.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=strategy)\n",
        "with strategy.scope():\n",
        "  tpu_model.build(input_shape = (None, 125, 125, 4))\n",
        "  tpu_model.compile(optimizer=optimiseur, steps_per_execution=exec_step, loss=criterion, metrics=[\"mse\"])\n",
        "\"\"\"\n",
        "model.summary()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fAhTLfwe3yoB",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title tensor loop\n",
        "# print(f\"E [{epoch + 1}/{epochs}], L: {running_loss:.4f}, T: [{datetime.datetime.now()}]\")\n",
        "#steps_per_epoch=steps_per_epoch\n",
        "batch_size_parq=50\n",
        "print(\"Training started at\", datetime.datetime.now())\n",
        "plotloss = []\n",
        "for epoch in range(epochs):\n",
        "  for parq in parqs:\n",
        "    parq_file = pq.ParquetFile(folder+parqs[0])\n",
        "    for i in parq_file.iter_batches(batch_size=batch_size_parq, columns=['X_jet', 'm']):\n",
        "      df = i.to_pandas()\n",
        "      X = df['X_jet'].values\n",
        "      y = df['m'].values\n",
        "      X = make_Xs(X)\n",
        "      print(X.shape)\n",
        "      history = model.fit(x=X, y=y, batch_size=batch_size_train, epochs=1, steps_per_epoch=steps_per_epoch, shuffle=True)\n",
        "      plotloss.append(sqrt(history.history['mse']))\n",
        "      print(datetime.datetime.now())\n",
        "print(\"Training completed at\", datetime.datetime.now())\n",
        "print(f\"Test RMSE: [{sqrt(model.evaluate(x=X_test, y=Y_test, batch_size=batch_size))}], T: [{datetime.datetime.now()}]\")"
      ],
      "metadata": {
        "id": "yLsCK_uVEUGr",
        "cellView": "form",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}