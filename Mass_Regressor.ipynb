{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bpratham2001/ResNet/blob/main/Mass_Regressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mass Regression\n",
        "\n",
        "Please train a model to estimate (regress) the mass of the particle based on particle images using the provided dataset.\n",
        "\n",
        "DataSet Description: 125x125 image matrices with name of variables: ieta and iphi, with 4 channels called X_jet (Track pT, DZ and D0, ECAL). Please use at least ECAL and Track pT channels and ‘am‘ as the target feature. If there are more than 4 channels in the dataset then you should use X_jet (Track pT, DZ and D0, ECAL) only. Please train your model on 80% of the data and evaluate on the remaining 20%. Please make sure not to overfit on the test dataset - it will be checked with an independent sample.\n",
        "\n",
        "Datasets: https://cernbox.cern.ch/s/zUvpkKhXIp0MJ0g\n",
        "\n",
        "**1,051,917 rows in total**"
      ],
      "metadata": {
        "id": "3HkhA62p43xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title !wget .parquet files\n",
        "\n",
        "#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_0.parquet\n",
        "#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_1.parquet\n",
        "#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_2.parquet\n",
        "#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_3.parquet\n",
        "#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_4.parquet\n",
        "#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_5.parquet\n",
        "#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_6.parquet\n",
        "\n",
        "parqs = ['top_gun_opendata_0.parquet', 'top_gun_opendata_1.parquet', 'top_gun_opendata_2.parquet',\n",
        "         'top_gun_opendata_3.parquet', 'top_gun_opendata_4.parquet', 'top_gun_opendata_5.parquet',\n",
        "         'top_gun_opendata_6.parquet']\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder = '/content/drive/My Drive/top_gun/'\n",
        "#folder = '/kaggle/working/'"
      ],
      "metadata": {
        "id": "-FPBtEKcRHhP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:26.958472Z",
          "iopub.execute_input": "2025-03-25T12:06:26.959092Z",
          "iopub.status.idle": "2025-03-25T12:06:26.966724Z",
          "shell.execute_reply.started": "2025-03-25T12:06:26.959060Z",
          "shell.execute_reply": "2025-03-25T12:06:26.965946Z"
        },
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36724e55-946c-4ad9-bd4f-8e8d70052243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "# @title imports and installs\n",
        "\n",
        "#import pyspark\n",
        "#from pyspark.sql import SparkSession\n",
        "#import pandas as pd\n",
        "#from IPython.display import clear_output\n",
        "import random\n",
        "import numpy as np\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import datetime\n",
        "from math import sqrt\n",
        "import gc\n",
        "#from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fCyVKUnYhvTD",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:26.967383Z",
          "iopub.execute_input": "2025-03-25T12:06:26.967566Z",
          "iopub.status.idle": "2025-03-25T12:06:28.798999Z",
          "shell.execute_reply.started": "2025-03-25T12:06:26.967547Z",
          "shell.execute_reply": "2025-03-25T12:06:28.797270Z"
        },
        "cellView": "form"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Model Architecture (ResNet)\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "  # This is the residual block, there are 4 residual blocks in ResNet15\n",
        "  def __init__(self, in_channels, out_channels, stride=1):\n",
        "    super(ResBlock, self).__init__()\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=stride, padding=0, bias=False)\n",
        "    nn.init.kaiming_normal_(self.conv1.weight)\n",
        "    self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    nn.init.kaiming_normal_(self.conv2.weight)\n",
        "    self.bn2 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "    self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "    nn.init.kaiming_normal_(self.conv3.weight)\n",
        "    self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      shortcut_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, bias=False)\n",
        "      nn.init.kaiming_normal_(shortcut_conv.weight)\n",
        "      self.shortcut.append(shortcut_conv)\n",
        "      self.shortcut.append(nn.BatchNorm2d(out_channels))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.conv3(out)\n",
        "    out = self.bn3(out)\n",
        "    out += self.shortcut(x)\n",
        "    out = self.relu(out)\n",
        "    return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, num_classes=2):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(4, 16, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "    nn.init.kaiming_normal_(self.conv1.weight)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    self.res1 = ResBlock(16, 32)\n",
        "    self.res2 = ResBlock(32, 64)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "    nn.init.kaiming_normal_(self.conv2.weight)\n",
        "    self.bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "    self.res3 = ResBlock(128, 256)\n",
        "    self.res4 = ResBlock(256, 512)\n",
        "\n",
        "    self.fc1 = nn.Linear(512 * 125 * 125, 1)\n",
        "    nn.init.kaiming_normal_(self.fc1.weight)\n",
        "    #self.fc2 = nn.Linear(16 * 125 * 125, 1)\n",
        "    #nn.init.kaiming_normal_(self.fc2.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.maxpool(out)\n",
        "    out = self.res1(out)\n",
        "    out = self.res2(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out = self.res3(out)\n",
        "    out = self.res4(out)\n",
        "    out = torch.flatten(out, 1)\n",
        "    out = self.fc1(out)\n",
        "    #out = self.fc2(out)\n",
        "    out = self.relu(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "K6JsswyMUuzm",
        "cellView": "form",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:28.799997Z",
          "iopub.execute_input": "2025-03-25T12:06:28.800322Z",
          "iopub.status.idle": "2025-03-25T12:06:28.814170Z",
          "shell.execute_reply.started": "2025-03-25T12:06:28.800297Z",
          "shell.execute_reply": "2025-03-25T12:06:28.812883Z"
        }
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "# @title dataset metadata and helper functions\n",
        "\n",
        "def get_parquet_rows(parq, start=0, end=-1, drive=False):\n",
        "  # since rows == row_groups in the dataset, we can use them interchangeably\n",
        "  lst = []\n",
        "  if drive:\n",
        "    parq_file = pq.ParquetFile(folder+parq)\n",
        "  else:\n",
        "    parq_file = pq.ParquetFile(parq)\n",
        "  if end < 0:\n",
        "    end = parq_file.scan_contents()\n",
        "  while start < end:\n",
        "    lst.append(parq_file.read_row_group(start))\n",
        "    start += 1\n",
        "  return lst\n",
        "\n",
        "def make_X(X_jet):\n",
        "  X_set = np.zeros(shape=(4, 125, 125))\n",
        "  for i in range(4): # only selecting the first 4 channels (Track pT, DZ, D0, ECAL)\n",
        "    for j in range(125):\n",
        "      for k in range(125):\n",
        "        X_set[i][j][k] = X_jet[i][j][k].astype(float)\n",
        "  return X_set\n",
        "  #return torch.tensor(X_set, dtype=torch.float32)\n",
        "\n",
        "def make_Xs(X_jets, chunk):\n",
        "  X_sets = np.zeros(shape=(chunk, 4, 125, 125))\n",
        "  for i in range(chunk):\n",
        "    for j in range(4):\n",
        "      for k in range(125):\n",
        "        for l in range(125):\n",
        "          X_sets[i][j][k][l] = X_jets[i][j][k][l].astype(float)\n",
        "  return X_sets\n",
        "\n",
        "\n",
        "def get_Xy(parq, start=0, end=-1, drive=False):\n",
        "  arr = np.array(get_parquet_rows(parq, start, end, drive))\n",
        "  X = []\n",
        "  y = []\n",
        "  for i in range(len(arr)):\n",
        "    X.append(make_X(np.array(arr[i][0][0])))\n",
        "    y.append(arr[i][0][1])\n",
        "  return np.array(X), np.array(y)\n",
        "\n",
        "\"\"\" # row counter\n",
        "tot = 0\n",
        "for parq in parqs:\n",
        "  num = pq.ParquetFile(folder+parq).scan_contents()\n",
        "  tot += num\n",
        "  print(parq+\" contains \"+str(num)+\" rows\")\n",
        "print(\"total \" + str(tot))\n",
        "del tot, num\n",
        "\n",
        "top_gun_opendata_0.parquet contains 150327 rows\n",
        "top_gun_opendata_1.parquet contains 150165 rows\n",
        "top_gun_opendata_2.parquet contains 150451 rows\n",
        "top_gun_opendata_3.parquet contains 150448 rows\n",
        "top_gun_opendata_4.parquet contains 150557 rows\n",
        "top_gun_opendata_5.parquet contains 150056 rows\n",
        "top_gun_opendata_6.parquet contains 149913 rows\n",
        "total 1051917\n",
        "\"\"\"\n",
        "\n",
        "for parq in parqs:\n",
        "  break\n",
        "  print(pq.read_metadata(folder+parq))\n",
        "  #print(pq.ParquetFile(folder+parq).read_row_group(0))\n",
        "\n",
        "class Deck():\n",
        "  def __init__(self, filenames, filesizes):\n",
        "    self.filenames = filenames\n",
        "    self.filesizes = filesizes\n",
        "    self.deck = []\n",
        "\n",
        "  def create_deck(self, max_chunk):\n",
        "    self.deck = []\n",
        "    for i in range(len(self.filenames)):\n",
        "      start = 0\n",
        "      while self.filesizes[i] > 0:\n",
        "        if max_chunk >= self.filesizes[i]:\n",
        "          self.deck.append([self.filenames[i], start, (start+self.filesizes[i])])\n",
        "          self.filesizes[i] = 0\n",
        "        else:\n",
        "          self.deck.append([self.filenames[i], start, (start+max_chunk)])\n",
        "          self.filesizes[i] -= max_chunk\n",
        "          start += max_chunk\n",
        "\n",
        "  def shuffle_deck(self):\n",
        "    random.shuffle(self.deck)\n",
        "\n",
        "  def deal_cards(self, split):\n",
        "    return self.deck[:int(len(self.deck)*split)], self.deck[int(len(self.deck)*split):]\n",
        "\n",
        "  def play_card(self, card, batch_size, drive=False, reshuffle=True):\n",
        "    if drive:\n",
        "      parq_file = pq.ParquetFile(folder+card[0])\n",
        "    else:\n",
        "      parq_file = pq.ParquetFile(card[0])\n",
        "\n",
        "    X = parq_file.read_row_groups(np.arange(card[1], card[2]), columns=['X_jet', 'm']).to_pandas()\n",
        "    torch.cuda.empty_cache()\n",
        "    y = torch.tensor(X['m'].to_numpy(), dtype=torch.float32)\n",
        "    X = make_Xs(X['X_jet'].to_numpy(), card[2]-card[1]) # Works, but slow\n",
        "    #X_tensor = torch.from_numpy(df['X_jet'].to_numpy(dtype=float))\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    #print(\"X_tensor\", str(X.shape))\n",
        "    #print(\"y_tensor\", str(y.shape))\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    return DataLoader(TensorDataset(X, y), batch_size=batch_size, shuffle=reshuffle)"
      ],
      "metadata": {
        "id": "hVZysWXXTHAb",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:28.815344Z",
          "iopub.execute_input": "2025-03-25T12:06:28.815567Z",
          "iopub.status.idle": "2025-03-25T12:06:34.778634Z",
          "shell.execute_reply.started": "2025-03-25T12:06:28.815545Z",
          "shell.execute_reply": "2025-03-25T12:06:34.777401Z"
        },
        "cellView": "form"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1/1000 : 029 seconds\n",
        "1/4000 : 125 seconds\n",
        "2/4000 : 120 seconds\n",
        "4/4000 :\n",
        "8/4000 :\n",
        "\"\"\"\n",
        "import multiprocessing\n",
        "def make_tensors(start):\n",
        "  parq_file = pq.ParquetFile(folder+parqs[1])\n",
        "  X = parq_file.read_row_groups(np.arange(start, start+1000), columns=['X_jet', 'm']).to_pandas()\n",
        "  y = torch.tensor(X['m'].to_numpy(), dtype=torch.float32)\n",
        "  X = torch.tensor(X['X_jet'].to_list(), dtype=torch.float32)\n",
        "  return X[:,:4], y\n",
        "\n",
        "print(datetime.datetime.now())\n",
        "\"\"\"\n",
        "s = [1000, 2000]\n",
        "parq_file = pq.ParquetFile(folder+parqs[1])\n",
        "X = parq_file.read_row_groups(np.arange(s[0], s[1]), columns=['X_jet', 'm']).to_pandas()\n",
        "y = torch.tensor(X['m'].to_numpy(), dtype=torch.float32)\n",
        "#X = make_Xs(X['X_jet'].to_numpy(), s[1]-s[0]) # Works, but slow\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  pool = multiprocessing.Pool()\n",
        "  pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
        "  #X = np.array(pool.map(make_X2, X['X_jet'].to_numpy()))\n",
        "  X = pool.map(make_tensors, [0, 1066, 2016, 3033])\n",
        "\n",
        "y = torch.cat([X[i][1] for i in range(4)],0)\n",
        "X = torch.cat([X[i][0] for i in range(4)],0)\n",
        "\n",
        "##outputs_async = pool.map_async(make_X, X['X_jet'].to_numpy())\n",
        "##outputs = outputs_async.get()\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(datetime.datetime.now())\n",
        "del X, y\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rmaxCQUhLaS",
        "outputId": "840cb21d-52b8-46bb-9854-d493b1603d11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-09 21:51:31.949105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-56d55c91d4c2>:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  X = torch.tensor(X['X_jet'].to_list(), dtype=torch.float32)\n",
            "<ipython-input-5-56d55c91d4c2>:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  X = torch.tensor(X['X_jet'].to_list(), dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4000, 4, 125, 125])\n",
            "torch.Size([4000])\n",
            "2025-04-09 21:53:33.707055\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MOKBMQMfy1Er"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import Pool\n",
        "print(datetime.datetime.now())\n",
        "if __name__ == '__main__':\n",
        "  with Pool(4) as p:\n",
        "    X = p.map(make_tensors, [0, 1066, 2016, 3033])\n",
        "    p.close()\n",
        "    p.join()\n",
        "y = torch.cat([X[i][1] for i in range(4)],0)\n",
        "X = torch.cat([X[i][0] for i in range(4)],0)\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "#X = torch.tensor(make_Xs(df['X_jet'].values, 4000), dtype=torch.float32) #10min\n",
        "print(datetime.datetime.now())\n",
        "del X, y\n",
        "gc.collect()\n",
        "#!pip uninstall -y tensorflow\n",
        "#!pip install tensorflow-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fl2gflwzXLN",
        "outputId": "a1495b99-cf0d-406b-82cd-023023c0f2e2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:34.779738Z",
          "iopub.execute_input": "2025-03-25T12:06:34.779985Z",
          "iopub.status.idle": "2025-03-25T12:06:39.053870Z",
          "shell.execute_reply.started": "2025-03-25T12:06:34.779961Z",
          "shell.execute_reply": "2025-03-25T12:06:39.052661Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-09 21:59:53.334982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-56d55c91d4c2>:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  X = torch.tensor(X['X_jet'].to_list(), dtype=torch.float32)\n",
            "<ipython-input-5-56d55c91d4c2>:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  X = torch.tensor(X['X_jet'].to_list(), dtype=torch.float32)\n",
            "Process ForkPoolWorker-8:\n",
            "Process ForkPoolWorker-11:\n",
            "Process ForkPoolWorker-7:\n",
            "Process ForkPoolWorker-12:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "           ^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 365, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "           ^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "           ^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
            "    task = get()\n",
            "           ^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "          ^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 395, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Tuning and Loading the Model\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "#import torch_xla.distributed.parallel_loader as pl\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "###############################################################################\n",
        "epochs = 2\n",
        "split = 0.8\n",
        "max_chunk = 2500\n",
        "batch_size = 250\n",
        "optimiseur = ['Adam', 0.001] # ['Adam, learning rate'] / ['SGD', learning rate, momentum] // scaled_eta = eta * xm.xrt_world_size()\n",
        "criterion = nn.MSELoss()#.to(device)\n",
        "path = '' # 'mass_regressor.pth'\n",
        "###############################################################################\n",
        "\n",
        "model = ResNet()\n",
        "mx = xmp.MpModelWrapper(model)\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "  print(\"Using \", torch.cuda.device_count(), \"GPUs\")\n",
        "  model = nn.DataParallel(model)\n",
        "\n",
        "if path != '':\n",
        "  model.load_state_dict(torch.load(f=path))"
      ],
      "metadata": {
        "id": "6Ei1S3sJT3NG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:39.055255Z",
          "iopub.execute_input": "2025-03-25T12:06:39.055562Z",
          "iopub.status.idle": "2025-03-25T12:06:44.766945Z",
          "shell.execute_reply.started": "2025-03-25T12:06:39.055532Z",
          "shell.execute_reply": "2025-03-25T12:06:44.765877Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title averageMeter\n",
        "class AverageMeter(object):\n",
        "\n",
        "    '''Computes and stores the average and current value'''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val   = 0\n",
        "        self.avg   = 0\n",
        "        self.sum   = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n = 1):\n",
        "        self.val    = val\n",
        "        self.sum   += val * n\n",
        "        self.count += n\n",
        "        self.avg    = self.sum / self.count"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rddg_wYUElIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = model.to(device)\n",
        "\n",
        "if optimiseur[0] == 'Adam':\n",
        "  optimiseur = torch.optim.Adam(model.parameters(),lr=optimiseur[1])\n",
        "elif optimiseur[0] == 'SGD':\n",
        "  optimiseur = torch.optim.SGD(model.parameters(), lr=optimiseur[1], momentum=optimiseur[2])\n",
        "\n",
        "param_size = 0\n",
        "for param in model.parameters():\n",
        "  param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in model.buffers():\n",
        "  buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "print('model size: {:.3f}MB'.format((param_size + buffer_size) / 1024**2))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:44.768314Z",
          "iopub.execute_input": "2025-03-25T12:06:44.768656Z",
          "iopub.status.idle": "2025-03-25T12:06:44.798189Z",
          "shell.execute_reply.started": "2025-03-25T12:06:44.768630Z",
          "shell.execute_reply": "2025-03-25T12:06:44.796610Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noMO4GP8C2q4",
        "outputId": "a593ef7b-2f72-4eb5-e370-13c8873efe9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 35.034MB\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training\n",
        "\n",
        "# para_loader = pl.ParallelLoader(valid_loader, [device])\n",
        "# para_loader = para_loader.per_device_loader(device)\n",
        "\n",
        "def train(model):\n",
        "  print(\"Training started at\", datetime.datetime.now())\n",
        "\n",
        "  # send to TPU\n",
        "  device = xm.xla_device()\n",
        "  model  = mx.to(device)\n",
        "\n",
        "  parq_len = [150327, 150165, 150451, 150448, 150557, 150056, 149913]\n",
        "  deck = Deck(parqs, parq_len)\n",
        "  deck.create_deck(max_chunk)\n",
        "  deck.shuffle_deck()\n",
        "  train_cards, test_cards = deck.deal_cards(split)\n",
        "  plotloss = []\n",
        "  flag = False\n",
        "  model.train()\n",
        "  trn_loss_meter = AverageMeter()\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimiseur, step_size = 1, gamma = 0.5)\n",
        "\n",
        "  gc.collect()\n",
        "  for epoch in range(epochs):\n",
        "    \"\"\"\n",
        "    if (epochs > 5) and (epoch > (epochs*0.5)):\n",
        "      optimiseur = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "    elif (epochs > 5) and (epoch > (epochs*0.75)):\n",
        "      optimiseur = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
        "    \"\"\"\n",
        "    #running_loss = 0.0 # MSE, per epoch\n",
        "\n",
        "    for card in train_cards:\n",
        "      #acc = [] # RMSE, per card per epoch\n",
        "      #print(card)\n",
        "      for i, (inputs, labels) in enumerate(deck.play_card(card, batch_size, drive=True, reshuffle=False)):\n",
        "        #print('{}MB'.format(inputs.nelement() * inputs.element_size() / 1024**2))\n",
        "        #print('{}MB'.format(labels.nelement() * labels.element_size() / 1024**2))\n",
        "        outputs = model(inputs)\n",
        "        if flag:\n",
        "          print(outputs) #torch.flatten(torch.rot90(outputs)))\n",
        "          print(labels) #torch.flatten(labels))\n",
        "          flag = False\n",
        "        loss = criterion(outputs, labels.unsqueeze(1))\n",
        "        optimiseur.zero_grad()\n",
        "        loss.backward()\n",
        "        #optimiseur.step()\n",
        "        xm.optimizer_step(optimiseur, barrier=True) # barrier is required on single-core training but can be dropped with multiple cores\n",
        "        #running_loss += loss.item()\n",
        "        trn_loss_meter.update(loss.detach().item(), inputs.size(0))\n",
        "        #acc.append(sqrt(loss))\n",
        "      #print(f\"RMSE: {sum(acc)/len(acc)}\")\n",
        "      xm.master_print('E{}, {} | loss = {:.6f}, avg_loss = {:.6f}'.format(epoch, card, loss.item(), trn_loss_meter.avg))\n",
        "\n",
        "      del inputs, labels, outputs, loss\n",
        "      gc.collect()\n",
        "      scheduler.step()\n",
        "    plotloss.append(trn_loss_meter.avg)\n",
        "    #print(f\"E [{epoch + 1}/{epochs}], L: {running_loss:.4f}, T: [{datetime.datetime.now()}]\")\n",
        "\n",
        "  print(\"Training completed at\", datetime.datetime.now())\n",
        "  return plotloss\n",
        "\n",
        "# wrapper function\n",
        "def _mp_fn(rank, flags):\n",
        "  torch.set_default_tensor_type('torch.FloatTensor')\n",
        "  trn_losses = train(model)\n",
        "  np.save('trn_losses.npy', np.array(trn_losses))\n",
        "  #np.save('val_losses.npy', np.array(val_losses))\n",
        "\n",
        "# modeling\n",
        "gc.collect()\n",
        "FLAGS = {}\n",
        "xmp.spawn(_mp_fn, args = (FLAGS,), nprocs = None, start_method = 'fork')"
      ],
      "metadata": {
        "id": "e-3iwoTfj_b-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T12:06:44.799452Z",
          "iopub.execute_input": "2025-03-25T12:06:44.799787Z",
          "iopub.status.idle": "2025-03-25T12:24:26.833909Z",
          "shell.execute_reply.started": "2025-03-25T12:06:44.799747Z",
          "shell.execute_reply": "2025-03-25T12:24:26.832156Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(x=np.array(list(range(epochs))), y=np.array(plotloss))"
      ],
      "metadata": {
        "id": "kzbPFbadz4rj",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Testing\n",
        "\n",
        "print(\"Testing started at\", datetime.datetime.now())\n",
        "\n",
        "val_loss = 0.0\n",
        "acc = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for card in test_cards:\n",
        "    for i, (inputs, labels) in enumerate(deck.play_card(card, batch_size, drive=True, reshuffle=False)):\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels.unsqueeze(1))\n",
        "      val_loss += loss.item()\n",
        "      acc.append(sqrt(loss))\n",
        "    print(sqrt(loss))\n",
        "\n",
        "print(f\"avg RMSE: {sum(acc)/len(acc)}\")\n",
        "print(\"Testing completed at\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "vkxrVrm7sqU7",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Saving\n",
        "torch.save(obj=model.state_dict(), f=\"resnet_regressor.pth\")"
      ],
      "metadata": {
        "id": "JL2JJoDNW4m7",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorflow stuffs to ignore"
      ],
      "metadata": {
        "id": "BAD-XNWc0D0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title imports and installs\n",
        "!pip install tensorflow==2.18.0\n",
        "!pip install tensorflow-tpu==2.18.0 --find-links=https://storage.googleapis.com/libtpu-tf-releases/index.html\n",
        "#import pyspark\n",
        "#from pyspark.sql import SparkSession\n",
        "#import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "#import tensorflow_datasets as tfds\n",
        "import datetime\n",
        "from math import sqrt\n",
        "#from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "O45LVjs13VUQ",
        "cellView": "form",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Tuning\n",
        "#TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "#optimiseur = keras.optimizers.SGD(learning_rate=0.1,momentum=0.9,decay = 1e-04)\n",
        "optimiseur = keras.optimizers.Adam(learning_rate=0.01)\n",
        "criterion = keras.losses.MeanSquaredError()\n",
        "batch_size_parq = 5000\n",
        "batch_size_train = 256\n",
        "steps_per_epoch = 50000 // batch_size_train\n",
        "exec_step = 50 # between 2 and steps_per_epoch\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "yyCKXwM1K3Ak",
        "cellView": "form",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title dataset metadata and helper functions\n",
        "\n",
        "def get_parquet_rows(parq, start=0, end=-1, drive=False):\n",
        "  # since rows == row_groups in the dataset, we can use them interchangeably\n",
        "  lst = []\n",
        "  if drive:\n",
        "    parq_file = pq.ParquetFile(folder+parq)\n",
        "  else:\n",
        "    parq_file = pq.ParquetFile(parq)\n",
        "  if end < 0:\n",
        "    end = parq_file.scan_contents()\n",
        "  while start < end:\n",
        "    lst.append(parq_file.read_row_group(start))\n",
        "    start += 1\n",
        "  return lst\n",
        "\n",
        "def make_X(X_jet):\n",
        "  X_set = np.zeros(shape=(4, 125, 125))\n",
        "  for i in range(4): # only selecting the first 4 channels (Track pT, DZ, D0, ECAL)\n",
        "    for j in range(125):\n",
        "      for k in range(125):\n",
        "        X_set[i][j][k] = X_jet[i][j][k]\n",
        "  return tf.convert_to_tensor(X_set, tf.float32)\n",
        "\n",
        "def make_Xs(X_jets):\n",
        "  X_sets = np.zeros(shape=(len(X_jets), 4, 125, 125))\n",
        "  for i in range(len(X_jets)):\n",
        "    for j in range(4): # only selecting the first 4 channels (Track pT, DZ, D0, ECAL)\n",
        "      for k in range(125):\n",
        "        for l in range(125):\n",
        "          X_sets[i][j][k][l] = X_jets[i][j][k][l]\n",
        "  return tf.convert_to_tensor(X_sets, tf.float32)\n",
        "\n",
        "\n",
        "def get_Xy(parq, start=0, end=-1, drive=False):\n",
        "  arr = np.array(get_parquet_rows(parq, start, end, drive))\n",
        "  X = []\n",
        "  y = []\n",
        "  for i in range(len(arr)):\n",
        "    X.append(make_X(np.array(arr[i][0][0])))\n",
        "    y.append(arr[i][0][1])\n",
        "  return np.array(X), np.array(y)\n",
        "\n",
        "\"\"\" # row counter\n",
        "tot = 0\n",
        "for parq in parqs:\n",
        "  num = pq.ParquetFile(folder+parq).scan_contents()\n",
        "  tot += num\n",
        "  print(parq+\" contains \"+str(num)+\" rows\")\n",
        "print(\"total \" + str(tot))\n",
        "del tot, num\n",
        "\n",
        "top_gun_opendata_0.parquet contains 150327 rows\n",
        "top_gun_opendata_1.parquet contains 150165 rows\n",
        "top_gun_opendata_2.parquet contains 150451 rows\n",
        "top_gun_opendata_3.parquet contains 150448 rows\n",
        "top_gun_opendata_4.parquet contains 150557 rows\n",
        "top_gun_opendata_5.parquet contains 150056 rows\n",
        "top_gun_opendata_6.parquet contains 149913 rows\n",
        "total 1051917\n",
        "\"\"\"\n",
        "parq_len = [150327, 150165, 150451, 150448, 150557, 150056, 149913]\n",
        "\n",
        "for parq in parqs:\n",
        "  print(pq.read_metadata(folder+parq))\n",
        "  #print(pq.ParquetFile(folder+parq).read_row_group(0))\n",
        "  break\n",
        "\n",
        "\"\"\"\n",
        "print(datetime.datetime.now())\n",
        "parq_file = pq.ParquetFile(folder+parqs[0])\n",
        "for i in parq_file.iter_batches(batch_size=4000, columns=['X_jet', 'm']):\n",
        "  df = i.to_pandas()\n",
        "  df['X_jet'] = df['X_jet'].map(lambda x: make_X(x))\n",
        "  break\n",
        "print(df['X_jet'].iloc[0].shape)\n",
        "print(datetime.datetime.now())\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FrAPIhCjKdgp",
        "cellView": "form",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title tf_recordwriter\n",
        "\n",
        "def read_tfrecord(example):\n",
        "  features={\n",
        "    \"X\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"y\": tf.io.FixedLenFeature([], tf.float32),\n",
        "    }\n",
        "  example = tf.io.parse_single_example(example, features)\n",
        "  #X = make_X(example[\"X_track_pT\", \"X_DZ\", \"X_D0\", \"X_ECAL\"])\n",
        "  X = tf.io.parse_tensor(example['X'], tf.float32)\n",
        "  y = tf.cast(example['y'], tf.float32)\n",
        "  return X, y\n",
        "\n",
        "def get_datasets(tfRecords, batch_size, shuffle=5000, multiple=False):\n",
        "  #For optimal performance, read multiple TFRecord files and set option experimental_deterministic = False\n",
        "  dataset = tf.data.TFRecordDataset(tfRecords, num_parallel_reads=tf.data.experimental.AUTOTUNE)\n",
        "  option = tf.data.Options()\n",
        "  if multiple:\n",
        "    option.experimental_deterministic = False\n",
        "  dataset = dataset.with_options(option)\n",
        "  dataset = dataset.map(read_tfrecord)\n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.shuffle(shuffle)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset\n",
        "\n",
        "with tf.io.TFRecordWriter(\"top_gun.tfrecords\") as file_writer:\n",
        "  print(\"Writing started at\", datetime.datetime.now())\n",
        "  for i in range(len(parqs)):\n",
        "    for j in range(parq_len[i]):\n",
        "      X, y = get_Xy(parqs[i], start=j, end=(j+4000), drive=True)\n",
        "      X_serialized = tf.io.serialize_tensor(X)\n",
        "      j += 5000\n",
        "      features = {\n",
        "          \"X\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[X_serialized.numpy()])),\n",
        "          \"y\": tf.train.Feature(float_list=tf.train.FloatList(value=y)),\n",
        "          }\n",
        "      features = tf.train.Features(feature=features)\n",
        "      example = tf.train.Example(features=features)\n",
        "      record_bytes = example.SerializeToString()\n",
        "      file_writer.write(record_bytes)\n",
        "      print(datetime.datetime.now())\n",
        "    print(f\"parq {parq[i]} complete\")\n",
        "  print(\"Writing completed at\", datetime.datetime.now())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3DnE1SW189sR",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ResNet18 Architecture\n",
        "\"\"\"\n",
        "ResNet-18\n",
        "Reference:\n",
        "[1] K. He et al. Deep Residual Learning for Image Recognition. CVPR, 2016\n",
        "[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers:\n",
        "Surpassing human-level performance on imagenet classification. In\n",
        "ICCV, 2015.\n",
        "\"\"\"\n",
        "\n",
        "class ResBlock(Model):\n",
        "  def __init__(self, channels, down_sample=False):\n",
        "    super().__init__()\n",
        "    self.__channels = channels\n",
        "    self.__down_sample = down_sample\n",
        "    self.__strides = [2, 1] if down_sample else [1, 1]\n",
        "\n",
        "    self.conv1 = Conv2D(self.__channels, strides=self.__strides[0], kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\")\n",
        "    self.bn1 = BatchNormalization()\n",
        "    self.conv2 = Conv2D(self.__channels * 2, strides=self.__strides[1], kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\")\n",
        "    self.bn2 = BatchNormalization()\n",
        "    self.merge = Add()\n",
        "    if self.__down_sample:\n",
        "      self.res_conv = Conv2D(self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=\"he_normal\", padding=\"same\")\n",
        "      self.res_bn = BatchNormalization()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    res = inputs\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.bn1(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    if self.__down_sample:\n",
        "      res = self.res_conv(res)\n",
        "      res = self.res_bn(res)\n",
        "    x = self.merge([x, res])\n",
        "    out = tf.nn.relu(x)\n",
        "    return out\n",
        "\n",
        "\n",
        "class ResNet(Model):\n",
        "  def __init__(self, **kwargs):\n",
        "    \"\"\"num_classes: number of classes in specific classification task.\"\"\"\n",
        "    super().__init__(**kwargs)\n",
        "    self.conv1 = Conv2D(64, (7, 7), strides=2, padding=\"same\", kernel_initializer=\"he_normal\")\n",
        "    self.bn = BatchNormalization()\n",
        "\n",
        "    self.maxpool = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
        "\n",
        "    self.res1_1 = ResBlock(64)\n",
        "    self.res1_2 = ResBlock(64)\n",
        "\n",
        "    self.res2_1 = ResBlock(128, down_sample=True)\n",
        "    self.res2_2 = ResBlock(128)\n",
        "\n",
        "    self.res3_1 = ResBlock(256, down_sample=True)\n",
        "    self.res3_2 = ResBlock(256)\n",
        "\n",
        "    self.res4_1 = ResBlock(512, down_sample=True)\n",
        "    self.res4_2 = ResBlock(512)\n",
        "\n",
        "    self.avgpool = GlobalAveragePooling2D()\n",
        "\n",
        "    self.fc1 = Dense(1, activation=\"relu\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "    out = self.conv1(inputs)\n",
        "    out = self.bn(out)\n",
        "    out = tf.nn.relu(out)\n",
        "    out = self.maxpool(out)\n",
        "    for res_block in [self.res1_1, self.res1_2, self.res2_1, self.res2_2, self.res3_1, self.res3_2, self.res4_1, self.res4_2]:\n",
        "      out = res_block(out)\n",
        "    out = self.avgpool(out)\n",
        "    out = Flatten(out)\n",
        "    out = self.fc1(out)\n",
        "    return out\n",
        "\n",
        "with strategy.scope():\n",
        "  model = ResNet()\n",
        "  model.build(input_shape = (None, 125, 125, 4))\n",
        "  model.compile(optimizer=optimiseur, steps_per_execution=exec_step, loss=criterion, metrics=[\"mse\"])\n",
        "\"\"\"\n",
        "model = ResNet()\n",
        "tpu_model = tf.compat.v1.estimator.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=strategy)\n",
        "with strategy.scope():\n",
        "  tpu_model.build(input_shape = (None, 125, 125, 4))\n",
        "  tpu_model.compile(optimizer=optimiseur, steps_per_execution=exec_step, loss=criterion, metrics=[\"mse\"])\n",
        "\"\"\"\n",
        "model.summary()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fAhTLfwe3yoB",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title tensor loop\n",
        "# print(f\"E [{epoch + 1}/{epochs}], L: {running_loss:.4f}, T: [{datetime.datetime.now()}]\")\n",
        "#steps_per_epoch=steps_per_epoch\n",
        "batch_size_parq=50\n",
        "print(\"Training started at\", datetime.datetime.now())\n",
        "plotloss = []\n",
        "for epoch in range(epochs):\n",
        "  for parq in parqs:\n",
        "    parq_file = pq.ParquetFile(folder+parqs[0])\n",
        "    for i in parq_file.iter_batches(batch_size=batch_size_parq, columns=['X_jet', 'm']):\n",
        "      df = i.to_pandas()\n",
        "      X = df['X_jet'].values\n",
        "      y = df['m'].values\n",
        "      X = make_Xs(X)\n",
        "      print(X.shape)\n",
        "      history = model.fit(x=X, y=y, batch_size=batch_size_train, epochs=1, steps_per_epoch=steps_per_epoch, shuffle=True)\n",
        "      plotloss.append(sqrt(history.history['mse']))\n",
        "      print(datetime.datetime.now())\n",
        "print(\"Training completed at\", datetime.datetime.now())\n",
        "print(f\"Test RMSE: [{sqrt(model.evaluate(x=X_test, y=Y_test, batch_size=batch_size))}], T: [{datetime.datetime.now()}]\")"
      ],
      "metadata": {
        "id": "yLsCK_uVEUGr",
        "cellView": "form",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}