{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"mount_file_id":"1lRKGYU9mgxne8oRME683IMJITi-Kiw7b","authorship_tag":"ABX9TyPz1gyJ65bGJmJjABve65gt","include_colab_link":true},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/bpratham2001/GSoC/blob/main/ML4SCI/mass-regressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"markdown","source":"# Mass Regression\n\nPlease train a model to estimate (regress) the mass of the particle based on particle images using the provided dataset.\n\nDataSet Description: 125x125 image matrices with name of variables: ieta and iphi, with 4 channels called X_jet (Track pT, DZ and D0, ECAL). Please use at least ECAL and Track pT channels and ‘am‘ as the target feature. If there are more than 4 channels in the dataset then you should use X_jet (Track pT, DZ and D0, ECAL) only. Please train your model on 80% of the data and evaluate on the remaining 20%. Please make sure not to overfit on the test dataset - it will be checked with an independent sample.\n\nDatasets: https://cernbox.cern.ch/s/zUvpkKhXIp0MJ0g\n\n**1,051,917 rows in total**","metadata":{"id":"3HkhA62p43xM"}},{"cell_type":"code","source":"# @title !wget .parquet files\n\n#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_0.parquet\n#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_1.parquet\n#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_2.parquet\n#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_3.parquet\n#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_4.parquet\n#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_5.parquet\n#!wget https://cernbox.cern.ch/remote.php/dav/public-files/zUvpkKhXIp0MJ0g/top_gun_opendata_6.parquet\n\nparqs = ['top_gun_opendata_0.parquet', 'top_gun_opendata_1.parquet', 'top_gun_opendata_2.parquet',\n         'top_gun_opendata_3.parquet', 'top_gun_opendata_4.parquet', 'top_gun_opendata_5.parquet',\n         'top_gun_opendata_6.parquet']\n#folder = '/content/drive/My Drive/top_gun/'\nfolder = '/kaggle/working/'","metadata":{"id":"-FPBtEKcRHhP","cellView":"form","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:06:26.958472Z","iopub.execute_input":"2025-03-25T12:06:26.959092Z","iopub.status.idle":"2025-03-25T12:06:26.966724Z","shell.execute_reply.started":"2025-03-25T12:06:26.959060Z","shell.execute_reply":"2025-03-25T12:06:26.965946Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# @title imports and installs\n\n#import pyspark\n#from pyspark.sql import SparkSession\n#import pandas as pd\n#from IPython.display import clear_output\nimport random\nimport numpy as np\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nimport datetime\nfrom math import sqrt\n#from sklearn.metrics import root_mean_squared_error\n\n#from google.colab import drive\n#drive.mount('/content/drive')","metadata":{"id":"fCyVKUnYhvTD","cellView":"form","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:06:26.967383Z","iopub.execute_input":"2025-03-25T12:06:26.967566Z","iopub.status.idle":"2025-03-25T12:06:28.798999Z","shell.execute_reply.started":"2025-03-25T12:06:26.967547Z","shell.execute_reply":"2025-03-25T12:06:28.797270Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# @title Model Architecture (ResNet)\n\nclass ResBlock(nn.Module):\n  # This is the residual block, there are 4 residual blocks in ResNet15\n  def __init__(self, in_channels, out_channels, stride=1):\n    super(ResBlock, self).__init__()\n    self.relu = nn.ReLU(inplace=True)\n    self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=stride, padding=0, bias=False)\n    nn.init.kaiming_normal_(self.conv1.weight)\n    self.bn1 = nn.BatchNorm2d(in_channels)\n\n    self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n    nn.init.kaiming_normal_(self.conv2.weight)\n    self.bn2 = nn.BatchNorm2d(in_channels)\n\n    self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n    nn.init.kaiming_normal_(self.conv3.weight)\n    self.bn3 = nn.BatchNorm2d(out_channels)\n\n    self.shortcut = nn.Sequential()\n\n    if stride != 1 or in_channels != out_channels:\n      shortcut_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, bias=False)\n      nn.init.kaiming_normal_(shortcut_conv.weight)\n      self.shortcut.append(shortcut_conv)\n      self.shortcut.append(nn.BatchNorm2d(out_channels))\n\n  def forward(self, x):\n    out = self.conv1(x)\n    out = self.bn1(out)\n    out = self.relu(out)\n    out = self.conv2(out)\n    out = self.bn2(out)\n    out = self.relu(out)\n    out = self.conv3(out)\n    out = self.bn3(out)\n    out += self.shortcut(x)\n    out = self.relu(out)\n    return out\n\nclass ResNet(nn.Module):\n  def __init__(self, num_classes=2):\n    super(ResNet, self).__init__()\n    self.conv1 = nn.Conv2d(4, 16, kernel_size=1, stride=1, padding=0, bias=False)\n    nn.init.kaiming_normal_(self.conv1.weight)\n    self.bn1 = nn.BatchNorm2d(16)\n    self.relu = nn.ReLU(inplace=True)\n\n    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n\n    self.res1 = ResBlock(16, 32)\n    self.res2 = ResBlock(32, 64)\n\n    self.conv2 = nn.Conv2d(64, 128, kernel_size=1, stride=1, padding=0, bias=False)\n    nn.init.kaiming_normal_(self.conv2.weight)\n    self.bn2 = nn.BatchNorm2d(128)\n\n    self.res3 = ResBlock(128, 256)\n    self.res4 = ResBlock(256, 512)\n\n    self.fc1 = nn.Linear(512 * 125 * 125, 1)\n    nn.init.kaiming_normal_(self.fc1.weight)\n    #self.fc2 = nn.Linear(16 * 125 * 125, 1)\n    #nn.init.kaiming_normal_(self.fc2.weight)\n\n  def forward(self, x):\n    out = self.conv1(x)\n    out = self.bn1(out)\n    out = self.relu(out)\n    out = self.maxpool(out)\n    out = self.res1(out)\n    out = self.res2(out)\n    out = self.conv2(out)\n    out = self.bn2(out)\n    out = self.res3(out)\n    out = self.res4(out)\n    out = torch.flatten(out, 1)\n    out = self.fc1(out)\n    #out = self.fc2(out)\n    out = self.relu(out)\n    return out","metadata":{"id":"K6JsswyMUuzm","cellView":"form","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:06:28.799997Z","iopub.execute_input":"2025-03-25T12:06:28.800322Z","iopub.status.idle":"2025-03-25T12:06:28.814170Z","shell.execute_reply.started":"2025-03-25T12:06:28.800297Z","shell.execute_reply":"2025-03-25T12:06:28.812883Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# @title dataset metadata and helper functions\n\ndef get_parquet_rows(parq, start=0, end=-1, drive=False):\n  # since rows == row_groups in the dataset, we can use them interchangeably\n  lst = []\n  if drive:\n    parq_file = pq.ParquetFile(folder+parq)\n  else:\n    parq_file = pq.ParquetFile(parq)\n  if end < 0:\n    end = parq_file.scan_contents()\n  while start < end:\n    lst.append(parq_file.read_row_group(start))\n    start += 1\n  return lst\n\ndef make_X(X_jet):\n  X_set = np.zeros(shape=(4, 125, 125))\n  for i in range(4): # only selecting the first 4 channels (Track pT, DZ, D0, ECAL)\n    for j in range(125):\n      for k in range(125):\n        X_set[i][j][k] = X_jet[i][j][k].astype(float)\n  return X_set\n  #return torch.tensor(X_set, dtype=torch.float32)\n\ndef make_Xs(X_jets, chunk):\n  X_sets = np.zeros(shape=(chunk, 4, 125, 125))\n  for i in range(chunk):\n    for j in range(4):\n      for k in range(125):\n        for l in range(125):\n          X_sets[i][j][k][l] = X_jets[i][j][k][l].astype(float)\n  return X_sets\n\n\ndef get_Xy(parq, start=0, end=-1, drive=False):\n  arr = np.array(get_parquet_rows(parq, start, end, drive))\n  X = []\n  y = []\n  for i in range(len(arr)):\n    X.append(make_X(np.array(arr[i][0][0])))\n    y.append(arr[i][0][1])\n  return np.array(X), np.array(y)\n\n\"\"\" # row counter\ntot = 0\nfor parq in parqs:\n  num = pq.ParquetFile(folder+parq).scan_contents()\n  tot += num\n  print(parq+\" contains \"+str(num)+\" rows\")\nprint(\"total \" + str(tot))\ndel tot, num\n\ntop_gun_opendata_0.parquet contains 150327 rows\ntop_gun_opendata_1.parquet contains 150165 rows\ntop_gun_opendata_2.parquet contains 150451 rows\ntop_gun_opendata_3.parquet contains 150448 rows\ntop_gun_opendata_4.parquet contains 150557 rows\ntop_gun_opendata_5.parquet contains 150056 rows\ntop_gun_opendata_6.parquet contains 149913 rows\ntotal 1051917\n\"\"\"\n\nfor parq in parqs:\n  print(pq.read_metadata(folder+parq))\n  #print(pq.ParquetFile(folder+parq).read_row_group(0))\n\nclass Deck():\n  def __init__(self, filenames, filesizes):\n    self.filenames = filenames\n    self.filesizes = filesizes\n    self.deck = []\n\n  def create_deck(self, max_chunk):\n    self.deck = []\n    for i in range(len(self.filenames)):\n      start = 0\n      while self.filesizes[i] > 0:\n        if max_chunk >= self.filesizes[i]:\n          self.deck.append([self.filenames[i], start, (start+self.filesizes[i])])\n          self.filesizes[i] = 0\n        else:\n          self.deck.append([self.filenames[i], start, (start+max_chunk)])\n          self.filesizes[i] -= max_chunk\n          start += max_chunk\n\n  def shuffle_deck(self):\n    random.shuffle(self.deck)\n\n  def deal_cards(self, split):\n    return self.deck[:int(len(self.deck)*split)], self.deck[int(len(self.deck)*split):]\n\n  def play_card(self, card, batch_size, drive=False, reshuffle=True):\n    if drive:\n      parq_file = pq.ParquetFile(folder+card[0])\n    else:\n      parq_file = pq.ParquetFile(card[0])\n\n    X = parq_file.read_row_groups(np.arange(card[1], card[2]), columns=['X_jet', 'm']).to_pandas()\n    torch.cuda.empty_cache()\n    y = torch.tensor(X['m'].to_numpy(), dtype=torch.float32)\n    X = make_Xs(X['X_jet'].to_numpy(), card[2]-card[1]) # Works, but slow\n    #X_tensor = torch.from_numpy(df['X_jet'].to_numpy(dtype=float))\n    X = torch.tensor(X, dtype=torch.float32)\n    #print(\"X_tensor\", str(X.shape))\n    #print(\"y_tensor\", str(y.shape))\n    X = X.to(device)\n    y = y.to(device)\n    return DataLoader(TensorDataset(X, y), batch_size=batch_size, shuffle=reshuffle)","metadata":{"id":"hVZysWXXTHAb","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:06:28.815344Z","iopub.execute_input":"2025-03-25T12:06:28.815567Z","iopub.status.idle":"2025-03-25T12:06:34.778634Z","shell.execute_reply.started":"2025-03-25T12:06:28.815545Z","shell.execute_reply":"2025-03-25T12:06:34.777401Z"}},"outputs":[{"name":"stdout","text":"<pyarrow._parquet.FileMetaData object at 0x7de0b29106d0>\n  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n  num_columns: 5\n  num_rows: 150327\n  num_row_groups: 150327\n  format_version: 1.0\n  serialized_size: 72239212\n<pyarrow._parquet.FileMetaData object at 0x7de0b2ac0ae0>\n  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n  num_columns: 5\n  num_rows: 150165\n  num_row_groups: 150165\n  format_version: 1.0\n  serialized_size: 72161492\n<pyarrow._parquet.FileMetaData object at 0x7de0b2aaa930>\n  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n  num_columns: 5\n  num_rows: 150451\n  num_row_groups: 150451\n  format_version: 1.0\n  serialized_size: 72300895\n<pyarrow._parquet.FileMetaData object at 0x7de0b29106d0>\n  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n  num_columns: 5\n  num_rows: 150448\n  num_row_groups: 150448\n  format_version: 1.0\n  serialized_size: 72299402\n<pyarrow._parquet.FileMetaData object at 0x7de0b292a750>\n  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n  num_columns: 5\n  num_rows: 150557\n  num_row_groups: 150557\n  format_version: 1.0\n  serialized_size: 72352622\n<pyarrow._parquet.FileMetaData object at 0x7de0b2aaa930>\n  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n  num_columns: 5\n  num_rows: 150056\n  num_row_groups: 150056\n  format_version: 1.0\n  serialized_size: 72110141\n<pyarrow._parquet.FileMetaData object at 0x7de0b2ac0ae0>\n  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n  num_columns: 5\n  num_rows: 149913\n  num_row_groups: 149913\n  format_version: 1.0\n  serialized_size: 72040028\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\"\"\"\nfrom multiprocessing import Pool\nprint(datetime.datetime.now())\nparq_file = pq.ParquetFile(folder+parqs[0])\ndf = parq_file.read_row_groups(np.arange(0, 4000), columns=['X_jet', 'm']).to_pandas()\nX = []\nwith Pool(4) as p:\n  X.append(p.map(make_X, df['X_jet'].values))\n  p.close()\n  p.join()\nlen(X)\n#X = torch.tensor(make_Xs(df['X_jet'].values, 4000), dtype=torch.float32) #10min\nprint(datetime.datetime.now())\n\"\"\"\n!pip uninstall -y tensorflow\n!pip install tensorflow-cpu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Fl2gflwzXLN","outputId":"aad46003-d2b3-4fd3-96fa-cf5fc06d2435","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:06:34.779738Z","iopub.execute_input":"2025-03-25T12:06:34.779985Z","iopub.status.idle":"2025-03-25T12:06:39.053870Z","shell.execute_reply.started":"2025-03-25T12:06:34.779961Z","shell.execute_reply":"2025-03-25T12:06:39.052661Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: tensorflow-cpu in /usr/local/lib/python3.10/site-packages (2.19.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (3.8.0)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (0.6.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (3.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (0.2.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.5.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.70.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (18.1.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (24.2)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (25.2.10)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (3.13.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.17.2)\nRequirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.0.2)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.6.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.32.3)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.17.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (4.12.2)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.1.0)\nRequirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.19.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (75.8.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (5.29.3)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (0.37.1)\nRequirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (0.5.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-cpu) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow-cpu) (13.9.4)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow-cpu) (0.14.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow-cpu) (0.0.8)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2025.1.31)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow-cpu) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow-cpu) (3.1.3)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow-cpu) (3.7)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow-cpu) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow-cpu) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow-cpu) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-cpu) (0.1.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# @title Tuning and Loading the Model\n\n#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n#import torch_xla.distributed.parallel_loader as pl\ndevice=xm.xla_device()\n###############################################################################\nepochs = 5\nsplit = 0.8\nmax_chunk = 10000\nbatch_size = 250\noptimiseur = ['Adam', 0.01] # ['Adam, learning rate'] / ['SGD', learning rate, momentum]\ncriterion = nn.MSELoss().to(device)\npath = '' # 'mass_regressor.pth'\n###############################################################################\n\nmodel = ResNet()\n\nif torch.cuda.device_count() > 1:\n  print(\"Using \", torch.cuda.device_count(), \"GPUs\")\n  model = nn.DataParallel(model)\n\nif path != '':\n  model.load_state_dict(torch.load(f=path))","metadata":{"id":"6Ei1S3sJT3NG","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:06:39.055255Z","iopub.execute_input":"2025-03-25T12:06:39.055562Z","iopub.status.idle":"2025-03-25T12:06:44.766945Z","shell.execute_reply.started":"2025-03-25T12:06:39.055532Z","shell.execute_reply":"2025-03-25T12:06:44.765877Z"}},"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1742904401.773716   10989 common_lib.cc:621] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:239\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model = model.to(device)\n\nif optimiseur[0] == 'Adam':\n  optimiseur = torch.optim.Adam(model.parameters(),lr=optimiseur[1])\nelif optimiseur[0] == 'SGD':\n  optimiseur = torch.optim.SGD(model.parameters(), lr=optimiseur[1], momentum=optimiseur[2])\n\nparam_size = 0\nfor param in model.parameters():\n  param_size += param.nelement() * param.element_size()\nbuffer_size = 0\nfor buffer in model.buffers():\n  buffer_size += buffer.nelement() * buffer.element_size()\n\nsize_all_mb = (param_size + buffer_size) / 1024**2\nprint('model size: {:.3f}MB'.format(size_all_mb))","metadata":{"id":"6Ei1S3sJT3NG","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:06:44.768314Z","iopub.execute_input":"2025-03-25T12:06:44.768656Z","iopub.status.idle":"2025-03-25T12:06:44.798189Z","shell.execute_reply.started":"2025-03-25T12:06:44.768630Z","shell.execute_reply":"2025-03-25T12:06:44.796610Z"}},"outputs":[{"name":"stdout","text":"model size: 35.034MB\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# @title Training\n\nprint(\"Training started at\", datetime.datetime.now())\nparq_len = [150327, 150165, 150451, 150448, 150557, 150056, 149913]\ndeck = Deck(parqs, parq_len)\ndeck.create_deck(max_chunk)\ndeck.shuffle_deck()\ntrain_cards, test_cards = deck.deal_cards(split)\nplotloss = []\nflag = False\nmodel.train()\nfor epoch in range(epochs):\n\n  if (epochs > 5) and (epoch > (epochs*0.5)):\n    optimiseur = torch.optim.Adam(model.parameters(),lr=0.001)\n  elif (epochs > 5) and (epoch > (epochs*0.75)):\n    optimiseur = torch.optim.Adam(model.parameters(),lr=0.0001)\n\n  running_loss = 0.0 # MSE, per epoch\n\n  for card in train_cards:\n    acc = [] # RMSE, per card per epoch\n    print(card)\n    for i, (inputs, labels) in enumerate(deck.play_card(card, batch_size, drive=True, reshuffle=False)):\n      #print('{}MB'.format(inputs.nelement() * inputs.element_size() / 1024**2))\n      #print('{}MB'.format(labels.nelement() * labels.element_size() / 1024**2))\n      outputs = model(inputs)\n      if flag:\n        print(outputs) #torch.flatten(torch.rot90(outputs)))\n        print(labels) #torch.flatten(labels))\n        flag = False\n      loss = criterion(outputs, labels.unsqueeze(1))\n      optimiseur.zero_grad()\n      loss.backward()\n      optimiseur.step()\n      xm.mark_step()\n      running_loss += loss.item()\n      acc.append(sqrt(loss))\n    print(f\"RMSE: {sum(acc)/len(acc)}\")\n  plotloss.append(running_loss)\n  print(f\"E [{epoch + 1}/{epochs}], L: {running_loss:.4f}, T: [{datetime.datetime.now()}]\")\n\nprint(\"Training completed at\", datetime.datetime.now())","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"id":"e-3iwoTfj_b-","outputId":"46f0ad74-6500-4d78-d344-ef49c1d99f4b","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:06:44.799452Z","iopub.execute_input":"2025-03-25T12:06:44.799787Z","iopub.status.idle":"2025-03-25T12:24:26.833909Z","shell.execute_reply.started":"2025-03-25T12:06:44.799747Z","shell.execute_reply":"2025-03-25T12:24:26.832156Z"}},"outputs":[{"name":"stdout","text":"Training started at 2025-03-25 12:06:44.807099\n['top_gun_opendata_0.parquet', 110000, 120000]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     35\u001b[0m optimiseur\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 36\u001b[0m \u001b[43mxm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmark_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     38\u001b[0m acc\u001b[38;5;241m.\u001b[39mappend(sqrt(loss))\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/core/xla_model.py:1046\u001b[0m, in \u001b[0;36mmark_step\u001b[0;34m(wait, reset_scope)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xu\u001b[38;5;241m.\u001b[39mgetenv_as(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXLA_EMIT_STEPLOG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1041\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m   1042\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch_xla.core.xla_model::mark_step\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1043\u001b[0m       end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1044\u001b[0m       file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1045\u001b[0m       flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1046\u001b[0m \u001b[43mtorch_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_XLAC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla_step_marker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_XLAC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla_get_default_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv_as\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mXLA_SYNC_WAIT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_scope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# Only emit metrics from the first local device index, to avoid emitting the\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# same values from different threads.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_master_ordinal():\n","\u001b[0;31mRuntimeError\u001b[0m: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 61.55G of 15.48G hbm. Exceeded hbm capacity by 46.06G.\n\nTotal hbm usage >= 62.06G:\n    reserved        530.00M \n    program          61.55G \n    arguments            0B \n\nOutput size 0B; shares 0B with arguments.\n\nProgram hbm requirement 61.55G:\n    HLO temp         61.55G (97.7% utilization: Unpadded (58.24G) Padded (59.64G), 3.1% fragmentation (1.91G))\n\n  Largest program allocations in hbm:\n\n  1. Size: 7.63G\n     Shape: f32[250,512,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 7.45G\n     Extra memory due to padding: 183.11M (1.0x expansion)\n     XLA label: fusion.41.remat5 = fusion(p85.1373, fusion.45.remat2.1.remat3, fusion.574, p78.1338, ...(+2)), kind=kOutput, calls=fused_computation.41.clone.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 7.63G\n     Shape: f32[250,512,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 7.45G\n     Extra memory due to padding: 183.11M (1.0x expansion)\n     XLA label: fusion.12 = fusion(fusion.41.remat5, get-tuple-element.1257, copy.62, get-tuple-element.1287, ...(+8)), kind=kLoop, calls=fused_computation.12\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 7.63G\n     Shape: f32[250,512,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 7.45G\n     Extra memory due to padding: 183.11M (1.0x expansion)\n     XLA label: fusion.31.remat.1.remat3 = fusion(p88.1404, p89.1405, fusion.395, get-tuple-element.1288, ...(+7)), kind=kOutput, calls=fused_computation.31.clone.1.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  4. Size: 7.63G\n     Shape: f32[250,512,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 7.45G\n     Extra memory due to padding: 183.11M (1.0x expansion)\n     XLA label: fusion.31.remat.1.remat3 = fusion(p88.1404, p89.1405, fusion.395, get-tuple-element.1288, ...(+7)), kind=kOutput, calls=fused_computation.31.clone.1.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  5. Size: 3.81G\n     Shape: f32[250,256,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 3.72G\n     Extra memory due to padding: 91.55M (1.0x expansion)\n     XLA label: fusion.45.remat3.1.remat = fusion(p80.1340, fusion.35.remat, fusion.575, p73.1301, ...(+2)), kind=kOutput, calls=fused_computation.45.clone.clone.clone.1.clone\n     Allocation type: HLO temp\n     ==========================\n\n  6. Size: 3.81G\n     Shape: f32[250,256,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 3.72G\n     Extra memory due to padding: 91.55M (1.0x expansion)\n     XLA label: fusion.105.remat6 = fusion(p65.1240, fusion.578, p58.1205, p59.1206, ...(+7)), kind=kOutput, calls=fused_computation.97.clone.clone.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  7. Size: 3.81G\n     Shape: f32[250,256,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 3.72G\n     Extra memory due to padding: 91.55M (1.0x expansion)\n     XLA label: fusion.1011.remat6 = fusion(fusion.105.remat4.1.remat6, fusion.576, fusion.577, p68.1271, ...(+12)), kind=kOutput, calls=fused_computation.36.clone.clone.clone.1.clone.clone.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  8. Size: 3.81G\n     Shape: f32[250,256,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 3.72G\n     Extra memory due to padding: 91.55M (1.0x expansion)\n     XLA label: fusion.35.remat10 = fusion(p75.1303, fusion.1011), kind=kOutput, calls=fused_computation.35.clone.clone.clone.clone.clone.clone.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  9. Size: 3.81G\n     Shape: f32[250,256,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 3.72G\n     Extra memory due to padding: 91.55M (1.0x expansion)\n     XLA label: fusion.715 = fusion(fusion.35.remat7, get-tuple-element.1311, p80.1340, gte.remat.28, ...(+9)), kind=kOutput, calls=fused_computation.664\n     Allocation type: HLO temp\n     ==========================\n\n  10. Size: 1.91G\n     Shape: f32[250,128,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 1.86G\n     Extra memory due to padding: 45.78M (1.0x expansion)\n     XLA label: fusion.100 = fusion(p55.1177, get-tuple-element.1356, fusion.580, p48.1138, ...(+2)), kind=kOutput, calls=fused_computation.92\n     Allocation type: HLO temp\n     ==========================\n\n  11. Size: 1.91G\n     Shape: bf16[250,256,125,125]{1,0,3,2:T(8,128)(2,1)}\n     Unpadded size: 1.86G\n     Extra memory due to padding: 45.78M (1.0x expansion)\n     XLA label: fusion.1011.remat6 = fusion(fusion.105.remat4.1.remat6, fusion.576, fusion.577, p68.1271, ...(+12)), kind=kOutput, calls=fused_computation.36.clone.clone.clone.1.clone.clone.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  12. Size: 1.91G\n     Shape: bf16[250,256,125,125]{1,0,3,2:T(8,128)(2,1)}\n     Unpadded size: 1.86G\n     Extra memory due to padding: 45.78M (1.0x expansion)\n     XLA label: fusion.1011.remat3 = fusion(fusion.105.remat4.1.remat2, fusion.576, fusion.577, p68.1271, ...(+12)), kind=kOutput, calls=fused_computation.36.clone.clone.clone.1.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  13. Size: 1.91G\n     Shape: pred[250,512,125,125]{1,0,3,2:T(8,128)(4,1)}\n     Unpadded size: 1.86G\n     Extra memory due to padding: 45.78M (1.0x expansion)\n     XLA label: fusion.12 = fusion(fusion.41.remat5, get-tuple-element.1257, copy.62, get-tuple-element.1287, ...(+8)), kind=kLoop, calls=fused_computation.12\n     Allocation type: HLO temp\n     ==========================\n\n  14. Size: 976.56M\n     Shape: f32[250,64,125,125]{0,1,3,2:T(8,128)}\n     Unpadded size: 953.67M\n     Extra memory due to padding: 22.89M (1.0x expansion)\n     XLA label: fusion.165 = fusion(get-tuple-element.1354, get-tuple-element.1355, fusion.581, fusion.582, ...(+6)), kind=kLoop, calls=fused_computation.149\n     Allocation type: HLO temp\n     ==========================\n\n  15. Size: 488.28M\n     Shape: f32[250,32,125,125]{0,1,3,2:T(8,128)}\n     Unpadded size: 476.84M\n     Extra memory due to padding: 11.44M (1.0x expansion)\n     XLA label: fusion.191 = fusion(get-tuple-element.1350, get-tuple-element.1351, fusion.585, fusion.586, ...(+6)), kind=kLoop, calls=fused_computation.175\n     Allocation type: HLO temp\n     ==========================\n\n  16. Size: 488.28M\n     Shape: f32[250,32,125,125]{0,1,3,2:T(8,128)}\n     Unpadded size: 476.84M\n     Extra memory due to padding: 11.44M (1.0x expansion)\n     XLA label: fusion.188 = fusion(p35.1044, get-tuple-element.1352, fusion.584, p28.1005, ...(+2)), kind=kOutput, calls=fused_computation.172\n     Allocation type: HLO temp\n     ==========================\n\n  17. Size: 244.14M\n     Shape: f32[250,16,125,125]{0,1,3,2:T(8,128)}\n     Unpadded size: 238.42M\n     Extra memory due to padding: 5.72M (1.0x expansion)\n     XLA label: fusion.254 = fusion(p15.911, get-tuple-element.1345, fusion.588, p8.785, ...(+2)), kind=kOutput, calls=fused_computation.234\n     Allocation type: HLO temp\n     ==========================\n\n  18. Size: 244.14M\n     Shape: f32[250,16,125,125]{0,1,3,2:T(8,128)}\n     Unpadded size: 238.42M\n     Extra memory due to padding: 5.72M (1.0x expansion)\n     XLA label: fusion.269 = fusion(get-tuple-element.1344, fusion.589, p2.3, p3.4, ...(+1)), kind=kLoop, calls=fused_computation.249\n     Allocation type: HLO temp\n     ==========================\n\n  19. Size: 31.25M\n     Shape: f32[512,125,125]{0,2,1:T(8,128)}\n     Unpadded size: 30.52M\n     Extra memory due to padding: 750.0K (1.0x expansion)\n     XLA label: copy.62 = copy(bitcast.9)\n     Allocation type: HLO temp\n     ==========================\n\n  20. Size: 2.25M\n     Shape: f32[256,256,3,3]{0,1,3,2:T(8,128)}\n     Unpadded size: 2.25M\n     XLA label: fusion.325 = fusion(copy.73, custom-call.3, copy.363, copy.364, ...(+15)), kind=kOutput, calls=fused_computation.286\n     Allocation type: HLO temp\n     ==========================\n\n"],"ename":"RuntimeError","evalue":"Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 61.55G of 15.48G hbm. Exceeded hbm capacity by 46.06G.\n\nTotal hbm usage >= 62.06G:\n    reserved        530.00M \n    program          61.55G \n    arguments            0B \n\nOutput size 0B; shares 0B with arguments.\n\nProgram hbm requirement 61.55G:\n    HLO temp         61.55G (97.7% utilization: Unpadded (58.24G) Padded (59.64G), 3.1% fragmentation (1.91G))\n\n  Largest program allocations in hbm:\n\n  1. Size: 7.63G\n     Shape: f32[250,512,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 7.45G\n     Extra memory due to padding: 183.11M (1.0x expansion)\n     XLA label: fusion.41.remat5 = fusion(p85.1373, fusion.45.remat2.1.remat3, fusion.574, p78.1338, ...(+2)), kind=kOutput, calls=fused_computation.41.clone.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 7.63G\n     Shape: f32[250,512,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 7.45G\n     Extra memory due to padding: 183.11M (1.0x expansion)\n     XLA label: fusion.12 = fusion(fusion.41.remat5, get-tuple-element.1257, copy.62, get-tuple-element.1287, ...(+8)), kind=kLoop, calls=fused_computation.12\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 7.63G\n     Shape: f32[250,512,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 7.45G\n     Extra memory due to padding: 183.11M (1.0x expansion)\n     XLA label: fusion.31.remat.1.remat3 = fusion(p88.1404, p89.1405, fusion.395, get-tuple-element.1288, ...(+7)), kind=kOutput, calls=fused_computation.31.clone.1.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  4. Size: 7.63G\n     Shape: f32[250,512,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 7.45G\n     Extra memory due to padding: 183.11M (1.0x expansion)\n     XLA label: fusion.31.remat.1.remat3 = fusion(p88.1404, p89.1405, fusion.395, get-tuple-element.1288, ...(+7)), kind=kOutput, calls=fused_computation.31.clone.1.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  5. Size: 3.81G\n     Shape: f32[250,256,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 3.72G\n     Extra memory due to padding: 91.55M (1.0x expansion)\n     XLA label: fusion.45.remat3.1.remat = fusion(p80.1340, fusion.35.remat, fusion.575, p73.1301, ...(+2)), kind=kOutput, calls=fused_computation.45.clone.clone.clone.1.clone\n     Allocation type: HLO temp\n     ==========================\n\n  6. Size: 3.81G\n     Shape: f32[250,256,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 3.72G\n     Extra memory due to padding: 91.55M (1.0x expansion)\n     XLA label: fusion.105.remat6 = fusion(p65.1240, fusion.578, p58.1205, p59.1206, ...(+7)), kind=kOutput, calls=fused_computation.97.clone.clone.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  7. Size: 3.81G\n     Shape: f32[250,256,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 3.72G\n     Extra memory due to padding: 91.55M (1.0x expansion)\n     XLA label: fusion.1011.remat6 = fusion(fusion.105.remat4.1.remat6, fusion.576, fusion.577, p68.1271, ...(+12)), kind=kOutput, calls=fused_computation.36.clone.clone.clone.1.clone.clone.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  8. Size: 3.81G\n     Shape: f32[250,256,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 3.72G\n     Extra memory due to padding: 91.55M (1.0x expansion)\n     XLA label: fusion.35.remat10 = fusion(p75.1303, fusion.1011), kind=kOutput, calls=fused_computation.35.clone.clone.clone.clone.clone.clone.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  9. Size: 3.81G\n     Shape: f32[250,256,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 3.72G\n     Extra memory due to padding: 91.55M (1.0x expansion)\n     XLA label: fusion.715 = fusion(fusion.35.remat7, get-tuple-element.1311, p80.1340, gte.remat.28, ...(+9)), kind=kOutput, calls=fused_computation.664\n     Allocation type: HLO temp\n     ==========================\n\n  10. Size: 1.91G\n     Shape: f32[250,128,125,125]{1,0,3,2:T(8,128)}\n     Unpadded size: 1.86G\n     Extra memory due to padding: 45.78M (1.0x expansion)\n     XLA label: fusion.100 = fusion(p55.1177, get-tuple-element.1356, fusion.580, p48.1138, ...(+2)), kind=kOutput, calls=fused_computation.92\n     Allocation type: HLO temp\n     ==========================\n\n  11. Size: 1.91G\n     Shape: bf16[250,256,125,125]{1,0,3,2:T(8,128)(2,1)}\n     Unpadded size: 1.86G\n     Extra memory due to padding: 45.78M (1.0x expansion)\n     XLA label: fusion.1011.remat6 = fusion(fusion.105.remat4.1.remat6, fusion.576, fusion.577, p68.1271, ...(+12)), kind=kOutput, calls=fused_computation.36.clone.clone.clone.1.clone.clone.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  12. Size: 1.91G\n     Shape: bf16[250,256,125,125]{1,0,3,2:T(8,128)(2,1)}\n     Unpadded size: 1.86G\n     Extra memory due to padding: 45.78M (1.0x expansion)\n     XLA label: fusion.1011.remat3 = fusion(fusion.105.remat4.1.remat2, fusion.576, fusion.577, p68.1271, ...(+12)), kind=kOutput, calls=fused_computation.36.clone.clone.clone.1.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  13. Size: 1.91G\n     Shape: pred[250,512,125,125]{1,0,3,2:T(8,128)(4,1)}\n     Unpadded size: 1.86G\n     Extra memory due to padding: 45.78M (1.0x expansion)\n     XLA label: fusion.12 = fusion(fusion.41.remat5, get-tuple-element.1257, copy.62, get-tuple-element.1287, ...(+8)), kind=kLoop, calls=fused_computation.12\n     Allocation type: HLO temp\n     ==========================\n\n  14. Size: 976.56M\n     Shape: f32[250,64,125,125]{0,1,3,2:T(8,128)}\n     Unpadded size: 953.67M\n     Extra memory due to padding: 22.89M (1.0x expansion)\n     XLA label: fusion.165 = fusion(get-tuple-element.1354, get-tuple-element.1355, fusion.581, fusion.582, ...(+6)), kind=kLoop, calls=fused_computation.149\n     Allocation type: HLO temp\n     ==========================\n\n  15. Size: 488.28M\n     Shape: f32[250,32,125,125]{0,1,3,2:T(8,128)}\n     Unpadded size: 476.84M\n     Extra memory due to padding: 11.44M (1.0x expansion)\n     XLA label: fusion.191 = fusion(get-tuple-element.1350, get-tuple-element.1351, fusion.585, fusion.586, ...(+6)), kind=kLoop, calls=fused_computation.175\n     Allocation type: HLO temp\n     ==========================\n\n  16. Size: 488.28M\n     Shape: f32[250,32,125,125]{0,1,3,2:T(8,128)}\n     Unpadded size: 476.84M\n     Extra memory due to padding: 11.44M (1.0x expansion)\n     XLA label: fusion.188 = fusion(p35.1044, get-tuple-element.1352, fusion.584, p28.1005, ...(+2)), kind=kOutput, calls=fused_computation.172\n     Allocation type: HLO temp\n     ==========================\n\n  17. Size: 244.14M\n     Shape: f32[250,16,125,125]{0,1,3,2:T(8,128)}\n     Unpadded size: 238.42M\n     Extra memory due to padding: 5.72M (1.0x expansion)\n     XLA label: fusion.254 = fusion(p15.911, get-tuple-element.1345, fusion.588, p8.785, ...(+2)), kind=kOutput, calls=fused_computation.234\n     Allocation type: HLO temp\n     ==========================\n\n  18. Size: 244.14M\n     Shape: f32[250,16,125,125]{0,1,3,2:T(8,128)}\n     Unpadded size: 238.42M\n     Extra memory due to padding: 5.72M (1.0x expansion)\n     XLA label: fusion.269 = fusion(get-tuple-element.1344, fusion.589, p2.3, p3.4, ...(+1)), kind=kLoop, calls=fused_computation.249\n     Allocation type: HLO temp\n     ==========================\n\n  19. Size: 31.25M\n     Shape: f32[512,125,125]{0,2,1:T(8,128)}\n     Unpadded size: 30.52M\n     Extra memory due to padding: 750.0K (1.0x expansion)\n     XLA label: copy.62 = copy(bitcast.9)\n     Allocation type: HLO temp\n     ==========================\n\n  20. Size: 2.25M\n     Shape: f32[256,256,3,3]{0,1,3,2:T(8,128)}\n     Unpadded size: 2.25M\n     XLA label: fusion.325 = fusion(copy.73, custom-call.3, copy.363, copy.364, ...(+15)), kind=kOutput, calls=fused_computation.286\n     Allocation type: HLO temp\n     ==========================\n\n","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"import seaborn as sns\nsns.lineplot(x=np.array(list(range(epochs))), y=np.array(plotloss))","metadata":{"id":"kzbPFbadz4rj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Testing\n\nprint(\"Testing started at\", datetime.datetime.now())\n\nval_loss = 0.0\nacc = []\nmodel.eval()\nwith torch.no_grad():\n  for card in test_cards:\n    for i, (inputs, labels) in enumerate(deck.play_card(card, batch_size, drive=True, reshuffle=False)):\n      outputs = model(inputs)\n      loss = criterion(outputs, labels.unsqueeze(1))\n      val_loss += loss.item()\n      acc.append(sqrt(loss))\n    print(sqrt(loss))\n\nprint(f\"avg RMSE: {sum(acc)/len(acc)}\")\nprint(\"Testing completed at\", datetime.datetime.now())","metadata":{"id":"vkxrVrm7sqU7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Saving\ntorch.save(obj=model.state_dict(), f=\"resnet_regressor.pth\")","metadata":{"id":"JL2JJoDNW4m7","cellView":"form","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tensorflow stuffs to ignore","metadata":{"id":"BAD-XNWc0D0f"}},{"cell_type":"code","source":"# @title imports and installs\n!pip install tensorflow==2.18.0\n!pip install tensorflow-tpu==2.18.0 --find-links=https://storage.googleapis.com/libtpu-tf-releases/index.html\n#import pyspark\n#from pyspark.sql import SparkSession\n#import pandas as pd\nfrom IPython.display import clear_output\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport os\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\nfrom keras.models import Sequential\nfrom keras.models import Model\n#import tensorflow_datasets as tfds\nimport datetime\nfrom math import sqrt\n#from sklearn.metrics import root_mean_squared_error\n\nfrom google.colab import drive\n#drive.mount('/content/drive')","metadata":{"id":"O45LVjs13VUQ","cellView":"form","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Tuning\n#TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))\nstrategy = tf.distribute.TPUStrategy(resolver)\n\n#optimiseur = keras.optimizers.SGD(learning_rate=0.1,momentum=0.9,decay = 1e-04)\noptimiseur = keras.optimizers.Adam(learning_rate=0.01)\ncriterion = keras.losses.MeanSquaredError()\nbatch_size_parq = 5000\nbatch_size_train = 256\nsteps_per_epoch = 50000 // batch_size_train\nexec_step = 50 # between 2 and steps_per_epoch\nepochs = 10","metadata":{"id":"yyCKXwM1K3Ak","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7a843478-a208-410f-af78-cf0988198dc4","cellView":"form","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title dataset metadata and helper functions\n\ndef get_parquet_rows(parq, start=0, end=-1, drive=False):\n  # since rows == row_groups in the dataset, we can use them interchangeably\n  lst = []\n  if drive:\n    parq_file = pq.ParquetFile(folder+parq)\n  else:\n    parq_file = pq.ParquetFile(parq)\n  if end < 0:\n    end = parq_file.scan_contents()\n  while start < end:\n    lst.append(parq_file.read_row_group(start))\n    start += 1\n  return lst\n\ndef make_X(X_jet):\n  X_set = np.zeros(shape=(4, 125, 125))\n  for i in range(4): # only selecting the first 4 channels (Track pT, DZ, D0, ECAL)\n    for j in range(125):\n      for k in range(125):\n        X_set[i][j][k] = X_jet[i][j][k]\n  return tf.convert_to_tensor(X_set, tf.float32)\n\ndef make_Xs(X_jets):\n  X_sets = np.zeros(shape=(len(X_jets), 4, 125, 125))\n  for i in range(len(X_jets)):\n    for j in range(4): # only selecting the first 4 channels (Track pT, DZ, D0, ECAL)\n      for k in range(125):\n        for l in range(125):\n          X_sets[i][j][k][l] = X_jets[i][j][k][l]\n  return tf.convert_to_tensor(X_sets, tf.float32)\n\n\ndef get_Xy(parq, start=0, end=-1, drive=False):\n  arr = np.array(get_parquet_rows(parq, start, end, drive))\n  X = []\n  y = []\n  for i in range(len(arr)):\n    X.append(make_X(np.array(arr[i][0][0])))\n    y.append(arr[i][0][1])\n  return np.array(X), np.array(y)\n\n\"\"\" # row counter\ntot = 0\nfor parq in parqs:\n  num = pq.ParquetFile(folder+parq).scan_contents()\n  tot += num\n  print(parq+\" contains \"+str(num)+\" rows\")\nprint(\"total \" + str(tot))\ndel tot, num\n\ntop_gun_opendata_0.parquet contains 150327 rows\ntop_gun_opendata_1.parquet contains 150165 rows\ntop_gun_opendata_2.parquet contains 150451 rows\ntop_gun_opendata_3.parquet contains 150448 rows\ntop_gun_opendata_4.parquet contains 150557 rows\ntop_gun_opendata_5.parquet contains 150056 rows\ntop_gun_opendata_6.parquet contains 149913 rows\ntotal 1051917\n\"\"\"\nparq_len = [150327, 150165, 150451, 150448, 150557, 150056, 149913]\n\nfor parq in parqs:\n  print(pq.read_metadata(folder+parq))\n  #print(pq.ParquetFile(folder+parq).read_row_group(0))\n  break\n\n\"\"\"\nprint(datetime.datetime.now())\nparq_file = pq.ParquetFile(folder+parqs[0])\nfor i in parq_file.iter_batches(batch_size=4000, columns=['X_jet', 'm']):\n  df = i.to_pandas()\n  df['X_jet'] = df['X_jet'].map(lambda x: make_X(x))\n  break\nprint(df['X_jet'].iloc[0].shape)\nprint(datetime.datetime.now())\n\"\"\"","metadata":{"id":"FrAPIhCjKdgp","cellView":"form","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title tf_recordwriter\n\ndef read_tfrecord(example):\n  features={\n    \"X\": tf.io.FixedLenFeature([], tf.string),\n    \"y\": tf.io.FixedLenFeature([], tf.float32),\n    }\n  example = tf.io.parse_single_example(example, features)\n  #X = make_X(example[\"X_track_pT\", \"X_DZ\", \"X_D0\", \"X_ECAL\"])\n  X = tf.io.parse_tensor(example['X'], tf.float32)\n  y = tf.cast(example['y'], tf.float32)\n  return X, y\n\ndef get_datasets(tfRecords, batch_size, shuffle=5000, multiple=False):\n  #For optimal performance, read multiple TFRecord files and set option experimental_deterministic = False\n  dataset = tf.data.TFRecordDataset(tfRecords, num_parallel_reads=tf.data.experimental.AUTOTUNE)\n  option = tf.data.Options()\n  if multiple:\n    option.experimental_deterministic = False\n  dataset = dataset.with_options(option)\n  dataset = dataset.map(read_tfrecord)\n  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n  dataset = dataset.shuffle(shuffle)\n  dataset = dataset.batch(batch_size)\n  return dataset\n\nwith tf.io.TFRecordWriter(\"top_gun.tfrecords\") as file_writer:\n  print(\"Writing started at\", datetime.datetime.now())\n  for i in range(len(parqs)):\n    for j in range(parq_len[i]):\n      X, y = get_Xy(parqs[i], start=j, end=(j+4000), drive=True)\n      X_serialized = tf.io.serialize_tensor(X)\n      j += 5000\n      features = {\n          \"X\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[X_serialized.numpy()])),\n          \"y\": tf.train.Feature(float_list=tf.train.FloatList(value=y)),\n          }\n      features = tf.train.Features(feature=features)\n      example = tf.train.Example(features=features)\n      record_bytes = example.SerializeToString()\n      file_writer.write(record_bytes)\n      print(datetime.datetime.now())\n    print(f\"parq {parq[i]} complete\")\n  print(\"Writing completed at\", datetime.datetime.now())","metadata":{"cellView":"form","id":"3DnE1SW189sR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title ResNet18 Architecture\n\"\"\"\nResNet-18\nReference:\n[1] K. He et al. Deep Residual Learning for Image Recognition. CVPR, 2016\n[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers:\nSurpassing human-level performance on imagenet classification. In\nICCV, 2015.\n\"\"\"\n\nclass ResBlock(Model):\n  def __init__(self, channels, down_sample=False):\n    super().__init__()\n    self.__channels = channels\n    self.__down_sample = down_sample\n    self.__strides = [2, 1] if down_sample else [1, 1]\n\n    self.conv1 = Conv2D(self.__channels, strides=self.__strides[0], kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\")\n    self.bn1 = BatchNormalization()\n    self.conv2 = Conv2D(self.__channels * 2, strides=self.__strides[1], kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\")\n    self.bn2 = BatchNormalization()\n    self.merge = Add()\n    if self.__down_sample:\n      self.res_conv = Conv2D(self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=\"he_normal\", padding=\"same\")\n      self.res_bn = BatchNormalization()\n\n  def call(self, inputs):\n    res = inputs\n    x = self.conv1(inputs)\n    x = self.bn1(x)\n    x = tf.nn.relu(x)\n    x = self.conv2(x)\n    x = self.bn2(x)\n    if self.__down_sample:\n      res = self.res_conv(res)\n      res = self.res_bn(res)\n    x = self.merge([x, res])\n    out = tf.nn.relu(x)\n    return out\n\n\nclass ResNet(Model):\n  def __init__(self, **kwargs):\n    \"\"\"num_classes: number of classes in specific classification task.\"\"\"\n    super().__init__(**kwargs)\n    self.conv1 = Conv2D(64, (7, 7), strides=2, padding=\"same\", kernel_initializer=\"he_normal\")\n    self.bn = BatchNormalization()\n\n    self.maxpool = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n\n    self.res1_1 = ResBlock(64)\n    self.res1_2 = ResBlock(64)\n\n    self.res2_1 = ResBlock(128, down_sample=True)\n    self.res2_2 = ResBlock(128)\n\n    self.res3_1 = ResBlock(256, down_sample=True)\n    self.res3_2 = ResBlock(256)\n\n    self.res4_1 = ResBlock(512, down_sample=True)\n    self.res4_2 = ResBlock(512)\n\n    self.avgpool = GlobalAveragePooling2D()\n\n    self.fc1 = Dense(1, activation=\"relu\")\n\n  def call(self, inputs):\n    out = self.conv1(inputs)\n    out = self.bn(out)\n    out = tf.nn.relu(out)\n    out = self.maxpool(out)\n    for res_block in [self.res1_1, self.res1_2, self.res2_1, self.res2_2, self.res3_1, self.res3_2, self.res4_1, self.res4_2]:\n      out = res_block(out)\n    out = self.avgpool(out)\n    out = Flatten(out)\n    out = self.fc1(out)\n    return out\n\nwith strategy.scope():\n  model = ResNet()\n  model.build(input_shape = (None, 125, 125, 4))\n  model.compile(optimizer=optimiseur, steps_per_execution=exec_step, loss=criterion, metrics=[\"mse\"])\n\"\"\"\nmodel = ResNet()\ntpu_model = tf.compat.v1.estimator.tpu.keras_to_tpu_model(\n    model,\n    strategy=strategy)\nwith strategy.scope():\n  tpu_model.build(input_shape = (None, 125, 125, 4))\n  tpu_model.compile(optimizer=optimiseur, steps_per_execution=exec_step, loss=criterion, metrics=[\"mse\"])\n\"\"\"\nmodel.summary()","metadata":{"cellView":"form","id":"fAhTLfwe3yoB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title tensor loop\n# print(f\"E [{epoch + 1}/{epochs}], L: {running_loss:.4f}, T: [{datetime.datetime.now()}]\")\n#steps_per_epoch=steps_per_epoch\nbatch_size_parq=50\nprint(\"Training started at\", datetime.datetime.now())\nplotloss = []\nfor epoch in range(epochs):\n  for parq in parqs:\n    parq_file = pq.ParquetFile(folder+parqs[0])\n    for i in parq_file.iter_batches(batch_size=batch_size_parq, columns=['X_jet', 'm']):\n      df = i.to_pandas()\n      X = df['X_jet'].values\n      y = df['m'].values\n      X = make_Xs(X)\n      print(X.shape)\n      history = model.fit(x=X, y=y, batch_size=batch_size_train, epochs=1, steps_per_epoch=steps_per_epoch, shuffle=True)\n      plotloss.append(sqrt(history.history['mse']))\n      print(datetime.datetime.now())\nprint(\"Training completed at\", datetime.datetime.now())\nprint(f\"Test RMSE: [{sqrt(model.evaluate(x=X_test, y=Y_test, batch_size=batch_size))}], T: [{datetime.datetime.now()}]\")","metadata":{"id":"yLsCK_uVEUGr","cellView":"form","trusted":true},"outputs":[],"execution_count":null}]}